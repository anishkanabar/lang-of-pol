# ################################
# Model: DeepSpeech 2 (fc + ctc)
# Authors: Eric Chandler 2022
#
# Notes: Just trying to get it running. Not even full arch yet.
#
# ################################

# Environment Parameters
seed: 1986
trial: 6
number_of_epochs: 2
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/train_deepspeech/seed_<seed>/trial_<trial>
save_folder: !ref <output_folder>/save

cluster: rcc   # 'ai' or 'rcc'
scratch_dir: /project/graziul/ra/echandler/scratch

# Data Parameters
dataset_name: atczero   #'librispeech' or 'police' or 'atczero'
data_sample_rate: 8000
sorting: ascending
num_train: null
num_sec: 750  # 10m training data

split_ratios:
    train: .8
    dev: .1
    test: .1

train_splits: ["train"]
dev_splits: ["dev"]
test_splits: ["test"]

train_csv: !ref <output_folder>/train.csv
valid_csv: !ref <output_folder>/dev.csv
test_csv: 
    - !ref <output_folder>/test.csv

# With data_parallel batch_size is split into N jobs
# With DDP batch_size is multiplied by N jobs
# Must be 3 per GPU to fit 32GB of VRAM
batch_size: 3
test_batch_size: 8

train_dataloader_opts:
   batch_size: !ref <batch_size>
valid_dataloader_opts:
   batch_size: !ref <batch_size>
test_dataloader_opts:
   batch_size: !ref <test_batch_size>


# Model Parameters
input_size: 660
hidden_size: 32
num_classes: 42  #  alpha, num, space, quote, blank, eos, bos, unk
model_sample_rate: 16000


# Decoding parameters
blank_index: 0
bos_index: 1
eos_index: 2

# Model Architecture
spec: !new:speechbrain.lobes.features.MFCC
spec_out_channels: 660

# SeanNaren uses HardTanh(0,20) as activation
conv1: !new:speechbrain.lobes.models.convolution.ConvBlock
  input_shape: [!ref <batch_size>, null, !ref <spec_out_channels>]
  out_channels: 32
  kernel_size: (11, 41)
  stride: (2, 2)
  num_layers: 1

conv2: !new:speechbrain.lobes.models.convolution.ConvBlock
  input_shape: [!ref <batch_size>, null, 32]
  out_channels: 32
  kernel_size: (11, 21)
  stride: (1, 1)  # the paper uses (1,2) which caused padding problems here
  num_layers: 1
  norm: !new:torch.nn.BatchNorm2d
    num_features: 32

convnet: !new:torch.nn.Sequential
  - !ref <conv1>
  - !ref <conv2>

final_linear: !new:torch.nn.Linear
  in_features: !ref <input_size>   # shoudl be <hidden_size>
  out_features: !ref <num_classes>
  bias: False

fc: !new:torch.nn.Sequential  # Should start with batch_norm over hidden_size
  - !ref <final_linear>
  - !new:speechbrain.nnet.activations.Softmax

# lookahead: !new:torch.nn.Sequential
#   - !new:speechbrain.nnet.activations.ReLU

#         self.lookahead = nn.Sequential(
#             # consider adding batch norm?
#             Lookahead(self.model_cfg.hidden_size, context=self.model_cfg.lookahead_context),
#             nn.Hardtanh(0, 20, inplace=True)
#         ) 

ctc_cost: !name:speechbrain.nnet.losses.ctc_loss
  blank_index: !ref <blank_index>

modules:
  spec: !ref <spec>
  conv1: !ref <conv1>
  conv2: !ref <conv2>
  fc: !ref <fc>

model: !new:torch.nn.ModuleList
  - [!ref <conv1>, !ref <conv2>, !ref <fc>]


# Optimizer
learning_rate: .1
optimizer: !name:torch.optim.SGD
  lr: !ref <learning_rate>

lr_annealing_model: !new:speechbrain.nnet.schedulers.NewBobScheduler
   initial_value: !ref <learning_rate>
   improvement_threshold: 0.0025
   annealing_factor: 0.8
   patient: 0


# Bookkeeping
wer_file: !ref <output_folder>/wer.txt
cer_file: !ref <output_folder>/cer.txt

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>

error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats

cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats
  split_tokens: True

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>
    scheduler_model: !ref <lr_annealing_model>
    counter: !ref <epoch_counter>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <output_folder>/train_log.txt


