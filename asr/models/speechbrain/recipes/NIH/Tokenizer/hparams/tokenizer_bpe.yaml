# ############################################################################
# Tokenizer: subword BPE with unigram 
# Training: NIH
# ############################################################################

trial: 11
vocab_size: 128
output_folder: !ref results/tokenizer/bpe<vocab_size>/trial_<trial>
train_log: !ref <output_folder>/train_log.txt

# Data files
environ: !include:../../hparams/params.yaml
cluster: !ref <environ[cluster]>
dataset_name: !ref <environ[dataset_name]>
num_sec: !ref <environ[num_sec]>
num_train: !ref <environ[num_train]>
split_ratios:
    train: .8
    dev: .2
train_csv: !ref <output_folder>/train.csv
valid_csv: !ref <output_folder>/dev.csv


# Training parameters
token_type: unigram  # ["unigram", "bpe", "char"]
# max runtime vocabulary size seems to depend on # of hours loaded
token_output: !ref <vocab_size>  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: transcript


tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train_csv>
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type> # ["unigram", "bpe", "char"]
   character_coverage: !ref <character_coverage>
   annotation_list_to_check: [!ref <train_csv>, !ref <valid_csv>]
