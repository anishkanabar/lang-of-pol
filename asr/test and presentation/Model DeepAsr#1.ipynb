{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excellent-fantasy",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2em; color:#F0A152;\"> DeepAsr is an open-source & Keras (Tensorflow) implementation of end-to-end Automatic Speech Recognition (ASR) engine and it supports multiple Speech Recognition architectures. <p>\n",
    "\n",
    "### Supported Asr Architectures:\n",
    "\n",
    "#### - Baidu's Deep Speech 2\n",
    "#### - DeepAsrNetwork1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-occupation",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.4em; color:#52C3F0;\"> This notebook tries to understand and rerun the DeepAsr raw model's code! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "irish-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "# the build-in deepasr model in local\n",
    "import deepasr as asr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# log document\n",
    "import logging\n",
    "from keras.callbacks import CSVLogger\n",
    "log = \"general.log\"\n",
    "logging.basicConfig(filename=log,level=logging.DEBUG,format='%(asctime)s %(message)s', datefmt='%d/%m/%Y %H:%M:%S')\n",
    "csv_logger = CSVLogger('model_log.csv', append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incomplete-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_trans(filepath, audio_type='.flac'):\n",
    "    \"\"\"\n",
    "    This function is to get audios and transcripts needed for training\n",
    "    @filepath: the path of the dicteory\n",
    "    \"\"\"\n",
    "    count, k, inp = 0, 0, []\n",
    "    audio_name, audio_trans = [], []\n",
    "    for dir1 in os.listdir(filepath):\n",
    "        if dir1 == '.DS_Store': continue\n",
    "        dir2_path = filepath + dir1 + '/'\n",
    "        for dir2 in os.listdir(dir2_path):\n",
    "            if dir2 == '.DS_Store': continue\n",
    "            dir3_path = dir2_path + dir2 + '/'\n",
    "            \n",
    "            for audio in os.listdir(dir3_path):\n",
    "                if audio.endswith('.txt'):\n",
    "                    k += 1\n",
    "                    trans_path = dir3_path + audio\n",
    "                    with open(trans_path) as f:\n",
    "                        line = f.readlines()\n",
    "                        for item in line:\n",
    "                            flac_path = dir3_path + item.split()[0] + audio_type\n",
    "                            audio_name.append(flac_path)\n",
    "                            \n",
    "                            text = item.split()[1:]\n",
    "                            text = ' '.join(text)\n",
    "                            audio_trans.append(text)\n",
    "    return pd.DataFrame({\"path\": audio_name, \"transcripts\": audio_trans})                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "destroyed-aruba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get some thing to train the model\n",
    "audio_trans = get_audio_trans('/Users/shiyang/Desktop/NIH//audio data/LibriSpeech/train-clean-100/')\n",
    "train_data = audio_trans[audio_trans['transcripts'].str.len() < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(feature_type = 'spectrogram', multi_gpu = False):\n",
    "    \"\"\"\n",
    "    Get the CTC pipeline\n",
    "    @feature_type: the format of our dataset\n",
    "    @multi_gpu: whether using multiple GPU\n",
    "    \"\"\"\n",
    "    # audio feature extractor, this is build on asr built-in methods\n",
    "    features_extractor = asr.features.preprocess(feature_type=feature_type, features_num=161,\n",
    "                                                 samplerate=16000,\n",
    "                                                 winlen=0.02,\n",
    "                                                 winstep=0.025,\n",
    "                                                 winfunc=np.hanning)\n",
    "    \n",
    "    # input label encoder\n",
    "    alphabet_en = asr.vocab.Alphabet(lang='en')\n",
    "    \n",
    "    # training model\n",
    "    model = asr.model.get_deepasrnetwork1(\n",
    "        input_dim=161,\n",
    "        output_dim=29,\n",
    "        is_mixed_precision=True\n",
    "    )\n",
    "    \n",
    "    # model optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-2,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-4\n",
    "    )\n",
    "    \n",
    "    # output label deocder\n",
    "    decoder = asr.decoder.GreedyDecoder()\n",
    "    \n",
    "    # CTC Pipeline\n",
    "    pipeline = asr.pipeline.ctc_pipeline.CTCPipeline(\n",
    "        alphabet=alphabet_en, features_extractor=features_extractor, model=model, optimizer=optimizer, decoder=decoder,\n",
    "        sample_rate=16000, mono=True, multi_gpu=multi_gpu\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-tutorial",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"DeepAsr\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          [(None, None, 161)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BN_1 (BatchNormalization)       (None, None, 161)    644         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_1 (Conv1D)               (None, None, 220)    177320      BN_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "CNBN_1 (BatchNormalization)     (None, None, 220)    880         Conv1D_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_2 (Conv1D)               (None, None, 220)    242220      CNBN_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "CNBN_2 (BatchNormalization)     (None, None, 220)    880         Conv1D_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, None, 512)    1127424     CNBN_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, None, 512)    1127424     CNBN_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 1024)   0           gru_1[0][0]                      \n",
      "                                                                 gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BN_2 (BatchNormalization)       (None, None, 1024)   4096        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 30)     30750       BN_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "the_output (TimeDistributed)    (None, None, 29)     899         time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           the_output[0][0]                 \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,712,537\n",
      "Trainable params: 2,709,287\n",
      "Non-trainable params: 3,250\n",
      "__________________________________________________________________________________________________\n",
      "Feature Extraction in progress...\n",
      "Feature Extraction completed.\n",
      "input features:  (3194, 593, 161)\n",
      "input labels:  (3194, 99)\n",
      "Model training initiated...\n",
      "Epoch 1/500\n"
     ]
    }
   ],
   "source": [
    "pipeline = get_config(feature_type='fbank', multi_gpu=False)\n",
    "history = pipeline.fit(train_dataset=train_data, batch_size=128, epochs=500, callbacks=[csv_logger])\n",
    "project_path = '/Users/shiyang/Desktop/NIH/'\n",
    "pipeline.save(project_path+'checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-sugar",
   "metadata": {},
   "source": [
    "### __Remark:__ *At this point, if everything run smoothly, we can achieve the traning model. I have tested this code on another computer and it works well. However, for mac m1, I still haven't figure out why the kernel automatically died out....*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-consortium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "tensor_flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
