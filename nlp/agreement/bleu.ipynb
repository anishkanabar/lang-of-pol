{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bleu Scores For Transcriptions\n",
    "\n",
    "Why BLEU? It's designed to score a hypothesis transcription against multiple expert transcriptions and uses higher order n-grams to approximate 'fluency' and other\n",
    "stylistic elements. If we use it to score the expert transcriptions amongst themselves, that should be a reasonable metric for transcriber agreement??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asr_dataset.police import BpcETL, AmbiguityStrategy\n",
    "from asr_dataset.constants import Cluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl = BpcETL(Cluster.RCC, \n",
    "    filter_inaudible=False, \n",
    "    filter_numeric=False, \n",
    "    filter_uncertain=False,\n",
    "    ambiguity=AmbiguityStrategy.ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = etl.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utterance Alignments\n",
    "\n",
    "The problem is that transcribers disagree on utterance existence and duration.\n",
    "The goal is to produce an alignment (mapping) that links utterances across transcribers.\n",
    "The solution is a monotonic alignment that increments when there's unanimous agreement the utterance is over. In other words, we merge utterances if there's any evidence they're connected.\n",
    "\n",
    "Algorithm:\n",
    "Every utterance starts in a singleton group. Sort groups by end time. Consider\n",
    "ith and (i-1th) group. If ith group overlaps previous group in time, merge them.\n",
    "Otherwise add ith utterance as new group. Continue until all groups are disjoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(end=data['offset']+data['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverInterval(pd.Interval):\n",
    "    \"\"\" An Interval that is considered equal to other overlapping Intervals.\"\"\"\n",
    "    def __lt__(self, other):\n",
    "        return other.left > self.right\n",
    "    def __gt__(self, other):\n",
    "        return self.left > other.right\n",
    "    def __le__(self, other):\n",
    "        return self < other or self == other\n",
    "    def __ge__(self, other):\n",
    "        return self > other or self == other\n",
    "    def __eq__(self, other):\n",
    "        return not self < other and not self > other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all utterances that overlap transitively,\n",
    "#   e.g. group a,b,c together if a & b and b & c regardless of a & c\n",
    "alignments = {}\n",
    "for tup in data.sort_values('end').itertuples():\n",
    "    row = tup._asdict()\n",
    "    current = OverInterval(row['offset'], row['end'])\n",
    "    intervals = alignments.get(row['original_audio'], [])\n",
    "    if intervals and intervals[-1].right >= current.left:\n",
    "        left = min(intervals[-1].left, current.left)\n",
    "        right = max(intervals[-1].right, current.right)\n",
    "        intervals[-1] = OverInterval(left, right)\n",
    "    else:\n",
    "        intervals.append(current)\n",
    "    alignments[row['original_audio']] = intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left, bisect_right\n",
    "\n",
    "def index(a, x):\n",
    "    'Locate the leftmost value exactly equal to x'\n",
    "    i = bisect_left(a, x)\n",
    "    if i != len(a) and a[i] == x:\n",
    "        return i\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligner(row):\n",
    "    iv = OverInterval(row['offset'], row['end'])\n",
    "    return index(alignments[row['original_audio']], iv)\n",
    "\n",
    "aligned = data.apply(aligner, axis=1)\n",
    "data = data.assign(alignment = aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter mono-transcriptions\n",
    "\n",
    "We can't compute agreement scores when only one transcriber saw the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scribers = data[['original_audio', 'transcriber']].drop_duplicates()\n",
    "scribecount = scribers.groupby('original_audio').count().rename(columns={'transcriber':'count'}).reset_index()\n",
    "multi_scribers = scribecount.loc[scribecount['count'] > 1, 'original_audio']\n",
    "scribers = scribers.loc[scribers['original_audio'].isin(multi_scribers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infill missing utterances\n",
    "\n",
    "After merging utterances, some transcribers still did not record speech during\n",
    "the merged interval. To get consistent corpus BLEU scores, we need to fill these\n",
    "in as empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_intervals = data[['original_audio', 'alignment']].drop_duplicates()\n",
    "corpus_prep = aud_intervals.merge(scribers) \\\n",
    "                    .merge(data, how='left') \\\n",
    "                    .assign(text = lambda x: x['text'].fillna(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use alignment to concatenate utterances and corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence-level data frame for sentence-level bleu score\n",
    "utterances = corpus_prep.sort_values('offset') \\\n",
    "            .groupby(['original_audio', \n",
    "                    'alignment',\n",
    "                    'transcriber']) \\\n",
    "            .agg({\"text\": \" \".join}) \\\n",
    "            .reset_index()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus-level data frame for corpus-level bleu score\n",
    "corpuses = utterances.sort_values('alignment') \\\n",
    "                    .groupby(['original_audio', 'transcriber']) \\\n",
    "                    .agg({\"text\": lambda x: list(x)}) \\\n",
    "                    .reset_index()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define BLEU\n",
    "\n",
    "We average over all audio files and using each transcriber as the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from nltk.translate.gleu_score import sentence_gleu, corpus_gleu\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "def sentence_metric(sents, mfunc):\n",
    "    \"\"\"Computes sentence level blue from list of candidate strings\"\"\"\n",
    "    metric = 0\n",
    "    assert len(sents) > 1\n",
    "    for i in range(len(sents)):\n",
    "        refs = [x.split() for x in sents]\n",
    "        hyp = refs.pop(i)\n",
    "        metric += mfunc(refs, hyp)\n",
    "    return metric / len(sents)\n",
    "\n",
    "def corpus_metric(corps, mfunc):\n",
    "    \"\"\"Computes sentence level blue from list of list of candidate strings\"\"\"\n",
    "    metric = 0\n",
    "    for i in range(len(corps)):\n",
    "        refs = [[sent.split() for sent in corp] for corp in corps]\n",
    "        hyp = refs.pop(i)\n",
    "        refs = [list(i) for i in zip(*refs)]  # collate sentences by author\n",
    "        metric += mfunc(refs, hyp)\n",
    "    return metric / len(corps)\n",
    "\n",
    "def sblue(sents):\n",
    "    return sentence_metric(sents, sentence_bleu)\n",
    "\n",
    "def sglue(sents):\n",
    "    return sentence_metric(sents, sentence_gleu)\n",
    "\n",
    "def cblue(corps):\n",
    "    return corpus_metric(corps, corpus_bleu)\n",
    "\n",
    "def cglue(corps):\n",
    "    return corpus_metric(corps, corpus_gleu)\n",
    "\n",
    "def score_sentence(utterances, text_col):\n",
    "    \"\"\" \n",
    "        Expects data frame of {original_audio, alignment, transcriber, text_col} \n",
    "        where text_col contains sentence strings\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        scores = utterances.groupby(['original_audio', 'alignment']) \\\n",
    "                        .agg({text_col: [sblue, sglue]}) \\\n",
    "                        .agg('mean')\n",
    "        print(scores)\n",
    "\n",
    "def score_corpus(corpuses, text_col):\n",
    "    \"\"\" \n",
    "        Expects data frame of {original_audio, transcriber, text_col} \n",
    "        where text_col contains a list of sentence strings\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        scores = corpuses.groupby(['original_audio']) \\\n",
    "                .agg({text_col: [cblue, cglue]}) \\\n",
    "                .agg('mean')\n",
    "        print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU\n",
    "\n",
    "**Takeaway** They're kinda low, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text  sblue    0.144809\n",
      "      sglue    0.330698\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "score_sentence(utterances, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text  cblue    0.335554\n",
      "      cglue    0.339263\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "score_corpus(corpuses, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat BLEU with word stemming\n",
    "\n",
    "**Takeaway** Stemming barely changes the scores, and is insensitive to stem algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "corpuses = corpuses.assign(stemmed = [[\" \".join([stemmer.stem(wrd) for wrd in sent.split()]) for sent in corp] for corp in corpuses['text']])\n",
    "utterances = utterances.assign(stemmed = [\" \".join([stemmer.stem(wrd) for wrd in sent.split()]) for sent in utterances['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed  sblue    0.148959\n",
      "         sglue    0.334716\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "score_sentence(utterances, 'stemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed  cblue    0.343932\n",
      "         cglue    0.346531\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "score_corpus(corpuses, 'stemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "corpuses = corpuses.assign(stemmed = [[\" \".join([stemmer.stem(wrd) for wrd in sent.split()]) for sent in corp] for corp in corpuses['text']])\n",
    "utterances = utterances.assign(stemmed = [\" \".join([stemmer.stem(wrd) for wrd in sent.split()]) for sent in utterances['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed  sblue    0.148435\n",
      "         sglue    0.334357\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "score_sentence(utterances, 'stemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed  cblue    0.340857\n",
      "         cglue    0.343985\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "score_corpus(corpuses, 'stemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75f1655cc98ebdec045ba0a94d0a3d9b40cce450c7e83a290e736f98858da139"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
