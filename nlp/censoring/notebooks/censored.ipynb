{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utterance Censorship\n",
    "\n",
    "In econometrics, 'censored' data is when you throw out observations that \n",
    "are correlated with your outcome. In this notebook we try to answer:\n",
    "if we throw out utterances that contain untranscribable or uncertain sections,\n",
    "does that bias the resulting language models? \n",
    "\n",
    "The hypothesis is yes: that\n",
    "there is a correlation between these texts and hard-to-hear/hard-to-pronounce phones/words and therefore possibly a correlation with semantics (meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if the cells aren't printing stuff\n",
    "# import logging\n",
    "# root_logger = logging.getLogger()\n",
    "# root_logger.setLevel(logging.DEBUG)\n",
    "# root_logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asr_dataset.police import BpcETL, AmbiguityStrategy\n",
    "from asr_dataset.constants import DataSizeUnit, Cluster\n",
    "\n",
    "cluster = Cluster['RCC']\n",
    "etl = BpcETL(cluster, filter_inaudible=False, filter_uncertain=False, filter_numeric=False, ambiguity=AmbiguityStrategy.ALL)\n",
    "# This should NOT throw errors about PySoundFile etc. \n",
    "# Sometimes it works sometimes it doesn't... I really hate midway :(\n",
    "data = etl.etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data as good/bad\n",
    "data = data.assign(inaudible = data['text'].str.contains('|'.join(etl.BAD_WORDS), regex=True, case=False),\n",
    "                    uncertain = lambda x: ~x['inaudible'] & x['text'].str.contains('\\[.+\\]', regex=True),\n",
    "                    clean = lambda x: ~x['inaudible'] & ~x['uncertain'])\n",
    "\n",
    "f\"{data['inaudible'].sum()} inaudible and {data['uncertain'].sum()} uncertain and {data['clean'].sum()} clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = {'clean': 0, 'uncertain': 1, 'inaudible': 2}\n",
    "unlabeler = {0: 'clean', 1:'uncertain', 2:'inaudible'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = pd.Series(np.zeros(len(data)))\n",
    "data.loc[data['inaudible'], 'label'] = labeler['inaudible']\n",
    "data.loc[data['clean'], 'label'] = labeler['clean']\n",
    "data.loc[data['uncertain'], 'label'] = labeler['uncertain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from collections import Counter\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "word_counters = {k:Counter() for k in labeler.values()}\n",
    "char_bigram_counters = {k:Counter() for k in labeler.values()}\n",
    "for tup in data.itertuples():\n",
    "    clean_txt = tup.text\n",
    "    for bad in etl.BAD_WORDS:\n",
    "        clean_txt = clean_txt.replace(bad, '')\n",
    "    clean_txt = clean_txt.replace('[','').replace(']','')\n",
    "    tokens = tokenizer(clean_txt)\n",
    "    word_counters[tup.label].update(tokens)\n",
    "    char_bigram_counters[tup.label].update(ngrams_iterator(clean_txt,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligned_counts(c1, c2):\n",
    "    pad1 = {k: c1.get(k, 0) for k in c1.keys() | c2.keys()}\n",
    "    pad2 = {k: c2.get(k, 0) for k in c1.keys() | c2.keys()}\n",
    "    return Counter(pad1), Counter(pad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_dist(c1, c2):\n",
    "    v1, v2 = aligned_counts(c1, c2)\n",
    "    return distance.jensenshannon(list(v1.values()), list(v2.values()))\n",
    "\n",
    "def corr_dist(c1, c2):\n",
    "    v1, v2 = aligned_counts(c1, c2)\n",
    "    return distance.correlation(list(v1.values()), list(v2.values()))\n",
    "\n",
    "def jacc_dist(c1, c2):\n",
    "    v1, v2 = aligned_counts(c1, c2)\n",
    "    b1, b2 = [v > 0 for v in v1.values()], [v > 0 for v in v2.values()]\n",
    "    return distance.jaccard(b1, b2)\n",
    "\n",
    "def jacc_sim(c1, c2):\n",
    "    return len(c1 & c2) / len(c1 | c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use laplace smoothing because word sets dont fully intersect\n",
    "def cross_entropy(c1, c2):\n",
    "    sum1, sum2 = sum(c1.values()), sum(c2.values())\n",
    "    freq1 = {k: v / sum1 for k,v in c1.items()}\n",
    "    freq2 = {k: v / sum2 for k,v in c2.items()}\n",
    "    ce = 0\n",
    "    for k in freq1:\n",
    "        ce -= freq1[k] * np.log(freq2.get(k, 0))\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "klfunc = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "def kl_div(c1, c2):\n",
    "    a1, a2 = aligned_counts(c1, c2)\n",
    "    e1 = {k: np.exp(v) for k, v in a1.items()}\n",
    "    e2 = {k: np.exp(v) for k, v in a2.items()}\n",
    "    sum1, sum2 = sum(e1.values()), sum(e2.values())\n",
    "    sm1 = {k: v / sum1 for k,v in e1.items()}\n",
    "    sm2 = {k: v / sum2 for k,v in e2.items()}\n",
    "    t1 = torch.tensor([np.log(v) for v in sm1.values()])\n",
    "    t2 = torch.tensor([np.log(v) for v in sm2.values()])\n",
    "    return klfunc(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corpora(counters):\n",
    "    for l1, c1 in counters.items():\n",
    "        for l2, c2 in counters.items():\n",
    "            k1, k2 = unlabeler[l1], unlabeler[l2]\n",
    "            if l1 >= l2:\n",
    "                continue\n",
    "            print(f'{k1} vs {k2}...')\n",
    "            loss = corr_dist(c1, c2)\n",
    "            print(f'\\t Correlation = {loss:.3f}')\n",
    "            loss = jacc_dist(c1, c2)\n",
    "            print(f'\\t Jaccard Dist = {loss:.3f}')\n",
    "            loss = js_dist(c1, c2)\n",
    "            print(f'\\t Jensen-Shannon = {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_corpora(word_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_corpora(char_bigram_counters)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5192df41692c1aa295b85ee46ec01a1c01b343417c94fcbc3e4a66f3c082fb33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
