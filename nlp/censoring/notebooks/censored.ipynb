{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "root_logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "police-extracted dataset stats:\n",
      "\tRow count = 62762\n",
      "\tMin duration = 0.01 (sec)\n",
      "\tMax duration = 24.93 (sec)\n",
      "\tMean duration = 2.27 (sec)\n",
      "\tStdev duration = 2.43 (sec)\n",
      "\tTotal duration = 1 days 15:38:58.708010\n",
      "Discarding 3407 missing audios.\n",
      "Discarding 287 too_short mp3s.\n",
      "Discarding 49 transcripts with no text.\n",
      "police-transformed dataset stats:\n",
      "\tRow count = 59019\n",
      "\tMin duration = 0.04 (sec)\n",
      "\tMax duration = 24.93 (sec)\n",
      "\tMean duration = 2.27 (sec)\n",
      "\tStdev duration = 2.42 (sec)\n",
      "\tTotal duration = 1 days 13:13:30.104800\n",
      "Writing utterance audio clips.\n",
      "Writing audio took 0:00:13.222853.\n",
      "Discarding 0 missing audios.\n",
      "Discarding 0 too_short mp3s.\n",
      "police-loaded dataset stats:\n",
      "\tRow count = 59019\n",
      "\tMin duration = 0.04 (sec)\n",
      "\tMax duration = 24.93 (sec)\n",
      "\tMean duration = 2.27 (sec)\n",
      "\tStdev duration = 2.42 (sec)\n",
      "\tTotal duration = 1 days 13:13:30.104800\n"
     ]
    }
   ],
   "source": [
    "from asr_dataset.police import BpcETL, AmbiguityStrategy\n",
    "from asr_dataset.constants import DataSizeUnit, Cluster\n",
    "\n",
    "cluster = Cluster['RCC']\n",
    "etl = BpcETL(cluster, filter_inaudible=False, filter_uncertain=False, filter_numeric=False, ambiguity=AmbiguityStrategy.ALL)\n",
    "data = etl.etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(inaudible = data['text'].str.contains('|'.join(etl.BAD_WORDS), regex=True, case=False),\n",
    "                    uncertain = lambda x: ~x['inaudible'] & x['text'].str.contains('\\[.+\\]', regex=True),\n",
    "                    clean = lambda x: ~x['inaudible'] & ~x['uncertain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7907 inaudible and 10099 uncertain and 41013 clean'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{data['inaudible'].sum()} inaudible and {data['uncertain'].sum()} uncertain and {data['clean'].sum()} clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = {'clean': 0, 'uncertain': 1, 'inaudible': 2}\n",
    "unlabeler = {0: 'clean', 1:'uncertain', 2:'inaudible'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = pd.Series(np.zeros(len(data)))\n",
    "data.loc[data['inaudible'], 'label'] = labeler['inaudible']\n",
    "data.loc[data['clean'], 'label'] = labeler['clean']\n",
    "data.loc[data['uncertain'], 'label'] = labeler['uncertain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from collections import Counter\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "word_counters = {k:Counter() for k in labeler.values()}\n",
    "char_bigram_counters = {k:Counter() for k in labeler.values()}\n",
    "for tup in data.itertuples():\n",
    "    clean_txt = tup.text\n",
    "    for bad in etl.BAD_WORDS:\n",
    "        clean_txt = clean_txt.replace(bad, '')\n",
    "    clean_txt = clean_txt.replace('[','').replace(']','')\n",
    "    tokens = tokenizer(clean_txt)\n",
    "    word_counters[tup.label].update(tokens)\n",
    "    char_bigram_counters[tup.label].update(ngrams_iterator(clean_txt,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligned_counts(c1, c2):\n",
    "    pad1 = {k: c1.get(k, 0) for k in c1.keys() | c2.keys()}\n",
    "    pad2 = {k: c2.get(k, 0) for k in c1.keys() | c2.keys()}\n",
    "    return Counter(pad1), Counter(pad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_dist(c1, c2):\n",
    "    v1, v2 = aligned_counts(c1, c2)\n",
    "    return distance.jensenshannon(list(v1.values()), list(v2.values()))\n",
    "\n",
    "def corr_dist(c1, c2):\n",
    "    v1, v2 = aligned_counts(c1, c2)\n",
    "    return distance.correlation(list(v1.values()), list(v2.values()))\n",
    "\n",
    "def jacc_dist(c1, c2):\n",
    "    v1, v2 = aligned_counts(c1, c2)\n",
    "    b1, b2 = [v > 0 for v in v1.values()], [v > 0 for v in v2.values()]\n",
    "    return distance.jaccard(b1, b2)\n",
    "\n",
    "def jacc_sim(c1, c2):\n",
    "    return len(c1 & c2) / len(c1 | c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use laplace smoothing because word sets dont fully intersect\n",
    "def cross_entropy(c1, c2):\n",
    "    sum1, sum2 = sum(c1.values()), sum(c2.values())\n",
    "    freq1 = {k: v / sum1 for k,v in c1.items()}\n",
    "    freq2 = {k: v / sum2 for k,v in c2.items()}\n",
    "    ce = 0\n",
    "    for k in freq1:\n",
    "        ce -= freq1[k] * np.log(freq2.get(k, 0))\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "klfunc = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "def kl_div(c1, c2):\n",
    "    a1, a2 = aligned_counts(c1, c2)\n",
    "    e1 = {k: np.exp(v) for k, v in a1.items()}\n",
    "    e2 = {k: np.exp(v) for k, v in a2.items()}\n",
    "    sum1, sum2 = sum(e1.values()), sum(e2.values())\n",
    "    sm1 = {k: v / sum1 for k,v in e1.items()}\n",
    "    sm2 = {k: v / sum2 for k,v in e2.items()}\n",
    "    t1 = torch.tensor([np.log(v) for v in sm1.values()])\n",
    "    t2 = torch.tensor([np.log(v) for v in sm2.values()])\n",
    "    return klfunc(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corpora(counters):\n",
    "    for l1, c1 in counters.items():\n",
    "        for l2, c2 in counters.items():\n",
    "            k1, k2 = unlabeler[l1], unlabeler[l2]\n",
    "            if l1 >= l2:\n",
    "                continue\n",
    "            print(f'{k1} vs {k2}...')\n",
    "            loss = corr_dist(c1, c2)\n",
    "            print(f'\\t Correlation = {loss:.3f}')\n",
    "            loss = jacc_dist(c1, c2)\n",
    "            print(f'\\t Jaccard Dist = {loss:.3f}')\n",
    "            loss = js_dist(c1, c2)\n",
    "            print(f'\\t Jensen-Shannon = {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean vs uncertain...\n",
      "\t Correlation = 0.081\n",
      "\t Jaccard Dist = 0.627\n",
      "\t Jensen-Shannon = 0.203\n",
      "clean vs inaudible...\n",
      "\t Correlation = 0.136\n",
      "\t Jaccard Dist = 0.684\n",
      "\t Jensen-Shannon = 0.241\n",
      "uncertain vs inaudible...\n",
      "\t Correlation = 0.013\n",
      "\t Jaccard Dist = 0.658\n",
      "\t Jensen-Shannon = 0.205\n"
     ]
    }
   ],
   "source": [
    "compare_corpora(word_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean vs uncertain...\n",
      "\t Correlation = 0.005\n",
      "\t Jaccard Dist = 0.279\n",
      "\t Jensen-Shannon = 0.075\n",
      "clean vs inaudible...\n",
      "\t Correlation = 0.014\n",
      "\t Jaccard Dist = 0.327\n",
      "\t Jensen-Shannon = 0.099\n",
      "uncertain vs inaudible...\n",
      "\t Correlation = 0.006\n",
      "\t Jaccard Dist = 0.248\n",
      "\t Jensen-Shannon = 0.070\n"
     ]
    }
   ],
   "source": [
    "compare_corpora(char_bigram_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5192df41692c1aa295b85ee46ec01a1c01b343417c94fcbc3e4a66f3c082fb33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
