{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drew Mini-Project\n",
    "\n",
    "_February 2022_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explore several questions relating to the characteristics of audio in different acquisition systems / zones / file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to play audio in browser\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: paths will likely need to be rewritten depending on location from which notebook is run\n",
    "\n",
    "# Zone 1 (farther - downtown)\n",
    "OLD1_RECORDING_PATH = \"/data/Zone1/2018_08_15/201808142342-953683-27730.mp3\"\n",
    "NEW1_RECORDING_PATH = \"/data/Zone1/2021_11_13/20211113_02-49-23-C 107.2 TO 06-51-46.mp3\"\n",
    "\n",
    "# try Zone 3 (close - Woodlawn)\n",
    "OLD3_RECORDING_PATH = \"/data/Zone3/2018_08_15/201808142347-288991-14545.mp3\"\n",
    "NEW3_RECORDING_PATH = \"/data/Zone3/2021_11_13/20211113_02-04-38-C 110.9 TO 04-57-17.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old (method) recording path - Zone 1: \n",
    "- First 10 seconds silence\n",
    "- With 30:12 left (20 seconds in), there's 10+ seconds of speech.\n",
    "\n",
    "Old (method) recording path - Zone 3:\n",
    "- First 10 seconds silence\n",
    "- With 30:12 left (19 seconds in), there's 10+ seconds of speech.\n",
    "\n",
    "\n",
    "New (method) recording path - Zone 1 and 3:\n",
    "- This new recording method certainly is running noise activity detection. Very little silence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Preparation of data and helper functions.\n",
    "#-------------------------------------------------------------------------------\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import requests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "[width, height] = matplotlib.rcParams['figure.figsize']\n",
    "if width < 10:\n",
    "  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n",
    "\n",
    "#### Information to fetch sample data\n",
    "\n",
    "# _SAMPLE_DIR = \"_sample_data\"\n",
    "# SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n",
    "# SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n",
    "\n",
    "# SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "# SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
    "\n",
    "# SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
    "# SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
    "\n",
    "# SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
    "# SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
    "\n",
    "# SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n",
    "# SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n",
    "\n",
    "# SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n",
    "# SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n",
    "\n",
    "# SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n",
    "# SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n",
    "# SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "# S3_BUCKET = \"pytorch-tutorial-assets\"\n",
    "# S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "# YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
    "# os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
    "# os.makedirs(_SAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "### Download sample data\n",
    "\n",
    "# def _fetch_data():\n",
    "#   uri = [\n",
    "#     (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n",
    "#     (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
    "#     (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
    "#     (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
    "#     (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n",
    "#     (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n",
    "#     (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n",
    "#   ]\n",
    "#   for url, path in uri:\n",
    "#     with open(path, 'wb') as file_:\n",
    "#       file_.write(requests.get(url).content)\n",
    "\n",
    "# _fetch_data()\n",
    "\n",
    "# def _download_yesno():\n",
    "#   if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n",
    "#     return\n",
    "#   torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n",
    "\n",
    "# YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n",
    "# YESNO_DOWNLOAD_PROCESS.start()\n",
    "\n",
    "### This appears to be used to fetch example samples exclusively, but I'll leave in just in case of wider applicability.\n",
    "\n",
    "def _get_sample(path, resample=None):\n",
    "  effects = [\n",
    "    [\"remix\", \"1\"]  # remix merges channels to 1\n",
    "  ]\n",
    "  if resample:  # resample converts sample rate via interpolation\n",
    "    effects.extend([\n",
    "      [\"lowpass\", f\"{resample // 2}\"],\n",
    "      [\"rate\", f'{resample}'],\n",
    "    ])\n",
    "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "### More functions to get specific samples\n",
    "\n",
    "# def get_speech_sample(*, resample=None):\n",
    "#   return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n",
    "\n",
    "# def get_sample(*, resample=None):\n",
    "#   return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "\n",
    "# def get_rir_sample(*, resample=None, processed=False):\n",
    "#   rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
    "#   if not processed:\n",
    "#     return rir_raw, sample_rate\n",
    "#   rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
    "#   rir = rir / torch.norm(rir, p=2)\n",
    "#   rir = torch.flip(rir, [1])\n",
    "#   return rir, sample_rate\n",
    "\n",
    "# def get_noise_sample(*, resample=None):\n",
    "#   return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
    "\n",
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "  if src:\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"-\" * 10)\n",
    "  if sample_rate:\n",
    "    print(\"Sample Rate:\", sample_rate)\n",
    "  print(\"Shape:\", tuple(waveform.shape))\n",
    "  print(\"Dtype:\", waveform.dtype)\n",
    "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "  print()\n",
    "  print(waveform)\n",
    "  print()\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "    axes[c].grid(True)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "    if ylim:\n",
    "      axes[c].set_ylim(ylim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def play_audio(waveform, sample_rate):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  if num_channels == 1:\n",
    "    display(Audio(waveform[0], rate=sample_rate))\n",
    "  elif num_channels == 2:\n",
    "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "  else:\n",
    "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
    "\n",
    "def inspect_file(path):\n",
    "  print(\"-\" * 10)\n",
    "  print(\"Source:\", path)\n",
    "  print(\"-\" * 10)\n",
    "  print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "  print(f\" - {torchaudio.info(path)}\")\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_mel_fbank(fbank, title=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Filter bank')\n",
    "  axs.imshow(fbank, aspect='auto')\n",
    "  axs.set_ylabel('frequency bin')\n",
    "  axs.set_xlabel('mel bin')\n",
    "  plt.show(block=False)\n",
    "\n",
    "### This is just an example\n",
    "\n",
    "# def get_spectrogram(\n",
    "#     n_fft = 400,\n",
    "#     win_len = None,\n",
    "#     hop_len = None,\n",
    "#     power = 2.0,\n",
    "# ):\n",
    "#   waveform, _ = get_speech_sample()\n",
    "#   spectrogram = T.Spectrogram(\n",
    "#       n_fft=n_fft,\n",
    "#       win_length=win_len,\n",
    "#       hop_length=hop_len,\n",
    "#       center=True,\n",
    "#       pad_mode=\"reflect\",\n",
    "#       power=power,\n",
    "#   )\n",
    "#   return spectrogram(waveform)\n",
    "\n",
    "def plot_pitch(waveform, sample_rate, pitch):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "\n",
    "  axis2.legend(loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Kaldi Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "  axis.set_ylim((-1.3, 1.3))\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n",
    "\n",
    "  lns = ln1 + ln2\n",
    "  labels = [l.get_label() for l in lns]\n",
    "  axis.legend(lns, labels, loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "DEFAULT_OFFSET = 201\n",
    "SWEEP_MAX_SAMPLE_RATE = 48000\n",
    "DEFAULT_LOWPASS_FILTER_WIDTH = 6\n",
    "DEFAULT_ROLLOFF = 0.99\n",
    "DEFAULT_RESAMPLING_METHOD = 'sinc_interpolation'\n",
    "\n",
    "def _get_log_freq(sample_rate, max_sweep_rate, offset):\n",
    "  \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n",
    "\n",
    "  offset is used to avoid negative infinity `log(offset + x)`.\n",
    "\n",
    "  \"\"\"\n",
    "  half = sample_rate // 2\n",
    "  start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n",
    "  return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n",
    "\n",
    "def _get_inverse_log_freq(freq, sample_rate, offset):\n",
    "  \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n",
    "  half = sample_rate // 2\n",
    "  return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n",
    "\n",
    "def _get_freq_ticks(sample_rate, offset, f_max):\n",
    "  # Given the original sample rate used for generating the sweep,\n",
    "  # find the x-axis value where the log-scale major frequency values fall in\n",
    "  time, freq = [], []\n",
    "  for exp in range(2, 5):\n",
    "    for v in range(1, 10):\n",
    "      f = v * 10 ** exp\n",
    "      if f < sample_rate // 2:\n",
    "        t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n",
    "        time.append(t)\n",
    "        freq.append(f)\n",
    "  t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n",
    "  time.append(t_max)\n",
    "  freq.append(f_max)\n",
    "  return time, freq\n",
    "\n",
    "def plot_sweep(waveform, sample_rate, title, max_sweep_rate=SWEEP_MAX_SAMPLE_RATE, offset=DEFAULT_OFFSET):\n",
    "  x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n",
    "  y_ticks = [1000, 5000, 10000, 20000, sample_rate//2]\n",
    "\n",
    "  time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n",
    "  freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n",
    "  freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n",
    "\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n",
    "  plt.xticks(time, freq_x)\n",
    "  plt.yticks(freq_y, freq_y)\n",
    "  axis.set_xlabel('Original Signal Frequency (Hz, log scale)')\n",
    "  axis.set_ylabel('Waveform Frequency (Hz)')\n",
    "  axis.xaxis.grid(True, alpha=0.67)\n",
    "  axis.yaxis.grid(True, alpha=0.67)\n",
    "  figure.suptitle(f'{title} (sample rate: {sample_rate} Hz)')\n",
    "  plt.show(block=True)\n",
    "\n",
    "def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n",
    "    max_sweep_rate = sample_rate\n",
    "    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n",
    "    delta = 2 * math.pi * freq / sample_rate\n",
    "    cummulative = torch.cumsum(delta, dim=0)\n",
    "    signal = torch.sin(cummulative).unsqueeze(dim=0)\n",
    "    return signal\n",
    "\n",
    "def benchmark_resample(\n",
    "    method,\n",
    "    waveform,\n",
    "    sample_rate,\n",
    "    resample_rate,\n",
    "    lowpass_filter_width=DEFAULT_LOWPASS_FILTER_WIDTH,\n",
    "    rolloff=DEFAULT_ROLLOFF,\n",
    "    resampling_method=DEFAULT_RESAMPLING_METHOD,\n",
    "    beta=None,\n",
    "    librosa_type=None,\n",
    "    iters=5\n",
    "):\n",
    "  if method == \"functional\":\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                 rolloff=rolloff, resampling_method=resampling_method)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"transforms\":\n",
    "    resampler = T.Resample(sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                           rolloff=rolloff, resampling_method=resampling_method, dtype=waveform.dtype)\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      resampler(waveform)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"librosa\":\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      librosa.resample(waveform_np, sample_rate, resample_rate, res_type=librosa_type)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll first look at an old-recording-method recording and a new-recording-method recording in each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old1_waveform, old1_sample_rate = torchaudio.load(OLD1_RECORDING_PATH)\n",
    "new1_waveform, new1_sample_rate = torchaudio.load(NEW1_RECORDING_PATH)\n",
    "old3_waveform, old3_sample_rate = torchaudio.load(OLD3_RECORDING_PATH)\n",
    "new3_waveform, new3_sample_rate = torchaudio.load(NEW3_RECORDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old3_waveform)\n",
    "print(new3_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torchaudio.info(OLD1_RECORDING_PATH))\n",
    "print(torchaudio.info(NEW1_RECORDING_PATH))\n",
    "print(torchaudio.info(OLD3_RECORDING_PATH))\n",
    "print(torchaudio.info(NEW3_RECORDING_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(old1_waveform[0])/(old1_sample_rate*60))\n",
    "print(len(new1_waveform[0])/(new1_sample_rate*60))\n",
    "print(len(old3_waveform[0])/(old3_sample_rate*60))\n",
    "print(len(new3_waveform[0])/(new3_sample_rate*60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only are the recordings different lengths (~30 min vs 2.5 min), they're at different sample rates. The new hardware samples at 48 kHz while the old audio sampled at 22.05 kHz.\n",
    "\n",
    "48 kHz is considered better bc 22 kHz causes aliasing down to 11 kHz, and humans can hear up to 15-20 kHz (https://www.ncbi.nlm.nih.gov/books/NBK10924/). However, it's a relatively small part of the perceptual range and probably doesn't have that much impact on speech recognition (https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00587/full) (let alone whether it even comes through in BPC or is attenuated). Maybe the most significant aspect of this change, then, is the change in noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old1_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old1_waveform_silence = torch.unsqueeze(old1_waveform[0][0:old1_sample_rate*10],0)  # identified as silence/noise via listening\n",
    "old1_waveform_speech = torch.unsqueeze(old1_waveform[0][old1_sample_rate*20:old1_sample_rate*30],0)  # identified as speech via listening\n",
    "old3_waveform_silence = torch.unsqueeze(old3_waveform[0][0:old3_sample_rate*10],0)  # identified as silence/noise via listening\n",
    "old3_waveform_speech = torch.unsqueeze(old3_waveform[0][old3_sample_rate*20:old3_sample_rate*30],0)  # identified as speech via listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(old1_waveform_silence[0]))\n",
    "print(len(old1_waveform_speech[0]))\n",
    "print(len(old3_waveform_silence[0]))\n",
    "print(len(old3_waveform_speech[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain waveform, silence\n",
    "plot_waveform(old1_waveform_silence, old1_sample_rate,title=\"old zone 1 silence\")\n",
    "plot_waveform(old3_waveform_silence, old3_sample_rate,title=\"old zone 3 silence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of zone 3 peaks is 10x higher than zone 1 peaks.\n",
    "\n",
    "Note the offset below 0 in Zone 1 and above 0 in Zone 3: what's that about?\n",
    "\n",
    "It seems strange to have DC offset differing between zones, because the same acquisition equipment was used. Possibly it's a < 0.1 Hz component of the signal? Not sure what could cause that either..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure offset\n",
    "print(f'old zone 1 silence dc offset: {torch.mean(old1_waveform_silence)}')\n",
    "print(f'old zone 3 silence dc offset: {torch.mean(old3_waveform_silence)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's less offset than appears, actually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'proportion points < 0: {len(old1_waveform_silence[old1_waveform_silence<0])/len(old1_waveform_silence[0])}')\n",
    "print(f'proportion points > 0: {len(old1_waveform_silence[old1_waveform_silence>0])/len(old1_waveform_silence[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the points must just be very close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain waveform, speech\n",
    "plot_waveform(old1_waveform_speech, old1_sample_rate,title=\"old zone 1 speech\")\n",
    "plot_waveform(old3_waveform_speech, old3_sample_rate,title=\"old zone 3 speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most intuitive difference between speech and silence of course shows up: speech is far more energetic, with peaks 100x higher.\n",
    "\n",
    "Zone 3 appears to be clipping a lot, if this plotting function worked correctly... but I'm not sure that's accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specgram, silence\n",
    "plot_specgram(old1_waveform_silence,old1_sample_rate,title=\"specgram - old zone 1 silence\")\n",
    "plot_specgram(old3_waveform_silence,old3_sample_rate,title=\"specgram - old zone 3 silence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specgram, speech\n",
    "plot_specgram(old1_waveform_speech,old1_sample_rate,title=\"specgram - old zone 1 speech\")\n",
    "plot_specgram(old3_waveform_speech,old3_sample_rate,title=\"specgram - old zone 3 speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear differences here. Unsurprisingly, the noise is far more uniform. The gap after 4 seconds in the speech spectrogram is a pause in talking. I believe the tall white noise (energy throughout the spectrum) spikes are the end-of-transmission sounds.\n",
    "\n",
    "One interesting distinction here: the silence's energy is almost all under 6kHz, while the speech has significant spikes above (perhaps mostly due to static). Most of the speech energy is clearly still below 5kHz or so.\n",
    "\n",
    "One other place I see a potentially interesting difference: right at the lowest frequencies, where there's a bit less energy in the speech sample but more energy in the silence sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram, silence\n",
    "\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "\n",
    "# define transformation\n",
    "spectrogram = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    ")\n",
    "# Perform transformation\n",
    "spec1si = spectrogram(old1_waveform_silence)\n",
    "\n",
    "print_stats(spec1si)\n",
    "plot_spectrogram(spec1si[0], title='spectrogram - old zone 1 silence')\n",
    "\n",
    "spec3si = spectrogram(old3_waveform_silence)\n",
    "\n",
    "print_stats(spec3si)\n",
    "plot_spectrogram(spec3si[0], title='spectrogram - old zone 3 silence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram, speech \n",
    "\n",
    "spec1sp = spectrogram(old1_waveform_speech)\n",
    "\n",
    "print_stats(spec1sp)\n",
    "plot_spectrogram(spec1sp[0], title='spectrogram - old zone 1 speech')\n",
    "\n",
    "spec3sp = spectrogram(old3_waveform_speech)\n",
    "\n",
    "print_stats(spec3sp)\n",
    "plot_spectrogram(spec3sp[0], title='spectrogram - old zone 3 speech')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the \"specgram\" (uses matplotlib specgram function) and \"spectrogram\" (uses torchaudio) outputs are very similar. Of note are the mostly constant horizontal lines (component tones) of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel spectrogram, silence\n",
    "\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 128\n",
    "\n",
    "mel_spectrogram_old = T.MelSpectrogram(\n",
    "    sample_rate=old1_sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    ")\n",
    "\n",
    "melspec1si = mel_spectrogram_old(old1_waveform_silence)\n",
    "plot_spectrogram(\n",
    "    melspec1si[0], title=\"MelSpectrogram - old zone 1 silence\", ylabel='mel freq')\n",
    "\n",
    "melspec1si = mel_spectrogram_old(old3_waveform_silence)\n",
    "plot_spectrogram(\n",
    "    melspec1si[0], title=\"MelSpectrogram - old zone 3 silence\", ylabel='mel freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something seems off in the mel frequency scale. Look into that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel spectrogram, speech\n",
    "\n",
    "melspec1sp = mel_spectrogram_old(old1_waveform_speech)\n",
    "plot_spectrogram(\n",
    "    melspec1sp[0], title=\"MelSpectrogram - old zone 1 speech\", ylabel='mel freq')\n",
    "\n",
    "melspec3sp = mel_spectrogram_old(old3_waveform_speech)\n",
    "plot_spectrogram(\n",
    "    melspec3sp[0], title=\"MelSpectrogram - old zone 3 speech\", ylabel='mel freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check out that low to high frequency difference! If this held true, that would indicate the ratio of low to mid to high frequency carries a lot of info about silence vs nonsilence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch, silence\n",
    "\n",
    "print(\"pitch - old zone 1 silence\")\n",
    "pitch1si = F.detect_pitch_frequency(old1_waveform_silence, old1_sample_rate)\n",
    "plot_pitch(old1_waveform_silence, old1_sample_rate, pitch1si)\n",
    "\n",
    "print(\"pitch - old zone 3 silence\")\n",
    "pitch3si = F.detect_pitch_frequency(old3_waveform_silence, old3_sample_rate)\n",
    "plot_pitch(old3_waveform_silence, old3_sample_rate, pitch1si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch, speech\n",
    "\n",
    "print(\"pitch - old zone 1 speech\")\n",
    "pitch1si = F.detect_pitch_frequency(old1_waveform_speech, old1_sample_rate)\n",
    "plot_pitch(old1_waveform_speech, old1_sample_rate, pitch1si)\n",
    "\n",
    "print(\"pitch - old zone 3 speech\")\n",
    "pitch3si = F.detect_pitch_frequency(old3_waveform_speech, old3_sample_rate)\n",
    "plot_pitch(old3_waveform_speech, old3_sample_rate, pitch1si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc, silence\n",
    "\n",
    "n_fft = 2048\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 256\n",
    "n_mfcc = 256\n",
    "\n",
    "mfcc_transform_old = T.MFCC(\n",
    "    sample_rate=old1_sample_rate,\n",
    "    n_mfcc=n_mfcc, melkwargs={'n_fft': n_fft, 'n_mels': n_mels, 'hop_length': hop_length})\n",
    "\n",
    "mfcc1si = mfcc_transform_old(old1_waveform_silence)\n",
    "plot_spectrogram(mfcc1si[0],title=\"mfcc - old zone 1 silence\")\n",
    "\n",
    "mfcc3si = mfcc_transform_old(old3_waveform_silence)\n",
    "plot_spectrogram(mfcc3si[0],title=\"mfcc - old zone 3 silence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc1sp = mfcc_transform_old(old1_waveform_speech)\n",
    "plot_spectrogram(mfcc1sp[0],title=\"mfcc - old zone 1 speech\")\n",
    "\n",
    "mfcc3sp = mfcc_transform_old(old3_waveform_speech)\n",
    "plot_spectrogram(mfcc3sp[0],title=\"mfcc - old zone 3 speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems clear that moving RMS on the plain waveform should generally be sufficient to detect audible sound vs silence. Moving ratios of bins of the spectrogram / mel spectrogram / MFCC can likely do the same, and perhaps even can distinguish some speech from other sounds. I will test the ratio approach here, basing it off of my observation that silence has relatively much stronger low-frequency vs middle-frequency components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec1si, spec3si, spec1sp, spec2sp\n",
    "# melspec1si, melspec3si, melspec1sp, melspec3sp\n",
    "# mfcc1si, mfcc3si, mfcc1sp, mfcc3sp\n",
    "\n",
    "# all have length 431 (frames), but different # of frequency bins:\n",
    "\n",
    "print(spec1si.shape)\n",
    "print(melspec1si.shape)\n",
    "print(mfcc1si.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at ratio of spectrogram bin mean values: 0-10:40-60\n",
    "\n",
    "# silence\n",
    "print(f\"Zone 1 silence ratio: {torch.mean(spec1si[0,0:10,:])/torch.mean(spec1si[0,40:60,:])}\")\n",
    "print(f\"Zone 3 silence ratio: {torch.mean(spec3si[0,0:10,:])/torch.mean(spec3si[0,40:60,:])}\")\n",
    "\n",
    "# speech (even including portions without speech)\n",
    "print(f\"Zone 1 speech ratio: {torch.mean(spec1sp[0,0:10,:])/torch.mean(spec1sp[0,40:60,:])}\")\n",
    "print(f\"Zone 3 speech ratio: {torch.mean(spec3sp[0,0:10,:])/torch.mean(spec3sp[0,40:60,:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old1_waveform_mixed = torch.unsqueeze(old1_waveform[0][0:old1_sample_rate*30],0)  # first 10 seconds silence, last 10 speech\n",
    "old3_waveform_mixed = torch.unsqueeze(old3_waveform[0][0:old3_sample_rate*30],0)  # first 10 seconds silence, last 10 speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spectrograms for each\n",
    "spec1mix = spectrogram(old1_waveform_mixed)\n",
    "plot_spectrogram(spec1mix[0])\n",
    "spec3mix = spectrogram(old3_waveform_mixed)\n",
    "plot_spectrogram(spec3mix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into 1-second slices along dimension 2\n",
    "slice1mix = spec1mix.unfold(2,43,43)\n",
    "# take mean of each bin for each second\n",
    "binmeans1 = slice1mix[0,:,:,:].mean(dim=2,keepdim=True)[:,:,0]\n",
    "# get bin ratio 0-10:40-60\n",
    "binratio1 = binmeans1[0:10,:].mean(dim=0,keepdim=True)[0,:]/binmeans1[40:60,:].mean(dim=0,keepdim=True)[0,:]\n",
    "plt.plot(binratio1)\n",
    "\n",
    "# repeat for zone 3\n",
    "slice3mix = spec3mix.unfold(2,43,43)\n",
    "binmeans3 = slice3mix[0,:,:,:].mean(dim=2,keepdim=True)[:,:,0]\n",
    "binratio3 = binmeans3[0:10,:].mean(dim=0,keepdim=True)[0,:]/binmeans3[40:60,:].mean(dim=0,keepdim=True)[0,:]\n",
    "plt.plot(binratio3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the second-by-second value of the ratio for each zone. We can see that the ratio of low- to middle-frequency components drops drastically right when speech begins in both recordings (remember that the first 20 seconds of each recording are silence, followed by roughly 10 seconds of speech). We can even see the momentary break in speech right at the start and end of the Zone 1 recording.\n",
    "\n",
    "If we wanted, we could shrink the size of the moving-average window or overlap it with itself, etc. But I'm not sure that would add much as we want to retain some of the silence buffer around the moments of active speech, as those play an important role in conversation (and would make transcription more difficult if stripped).\n",
    "\n",
    "An expansion on this approach would be to also look at the low/middle to high frequency ratio. This could be used to identify bursts of white noise such as radio squawks, which may have further downstream uses such as normalizing between zones. In general, by averaging a range of frequencies that are known to have high power in human voice, and averaging other frequencies that do not, and comparing their ratio, we may be able to distinguish speech from nonspeech sounds from silence. I am not sure that using mel-scale frequency would make this any more effective, and suspect that it could make it less effective."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
