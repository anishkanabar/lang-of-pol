{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa57810aab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/project/graziul/data/Zone1/2018_08_04/2018_08_04vad_dict.pkl','rb')\n",
    "vad_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pydub': {-24: {'nonsilent_slices': [[392854, 394170],\n",
       "    [395263, 395267],\n",
       "    [397620, 398701],\n",
       "    [399578, 409655],\n",
       "    [410254, 410656],\n",
       "    [411215, 411220],\n",
       "    [414952, 416884],\n",
       "    [418455, 418461],\n",
       "    [824867, 825037],\n",
       "    [825665, 826898],\n",
       "    [828423, 828427],\n",
       "    [829500, 830273],\n",
       "    [830861, 838611],\n",
       "    [839113, 842452],\n",
       "    [843753, 843953],\n",
       "    [845021, 845763],\n",
       "    [846627, 850160],\n",
       "    [850738, 851675],\n",
       "    [852412, 853170],\n",
       "    [853794, 855787],\n",
       "    [856386, 858849],\n",
       "    [859979, 859982],\n",
       "    [867465, 868866],\n",
       "    [870249, 871474],\n",
       "    [872242, 873340],\n",
       "    [874666, 874864],\n",
       "    [875933, 879834],\n",
       "    [880956, 880960],\n",
       "    [1631361, 1633400],\n",
       "    [1634783, 1634786],\n",
       "    [1635967, 1638723],\n",
       "    [1639276, 1639316],\n",
       "    [1640028, 1644938],\n",
       "    [1646046, 1649121],\n",
       "    [1650149, 1650152]],\n",
       "   'nonsilent_minutes': 0.9895676492819349}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_key = list(vad_dict.keys())[0]\n",
    "vad_dict[test_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = audio_file(test_key)\n",
    "a.get_slices(vad_dict)\n",
    "input_list.append(a.get_mfcc()[:,:,:180409])\n",
    "a.get_frames()\n",
    "labels_list.append(a.get_labels()[:180409])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 183714])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = a.get_mfcc()\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_audio(datafile, div_size = 29): #Divide the audio clip into bits of 1 minute each\n",
    "#resizes input arrays from (1,feature_length, time) to (div_size,feature_length,time/div_length)\n",
    "    return np.reshape(datafile,[div_size,datafile.shape[1],datafile.shape[2]//div_size])\n",
    "\n",
    "class audio_file():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.vad_slices = None\n",
    "        self.frames = None\n",
    "        self.frames_labels = None\n",
    "        self.mfcc = None\n",
    "    \n",
    "    def get_slices(self, vad_dict):\n",
    "        self.vad_slices = vad_dict[self.name]['pydub'][-24]['nonsilent_slices']\n",
    "        return self.vad_slices\n",
    "    \n",
    "    def get_frames(self):\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2])\n",
    "        #frames_array = np.zeros(180409)\n",
    "\n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "\n",
    "            for i in range(start,end):\n",
    "                n = math.floor(i/220)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "            \n",
    "            self.frames = frames_array\n",
    "            return self.frames\n",
    "        \n",
    "    def get_labels(self): \n",
    "        self.frames_labels = np.zeros(len(self.frames))\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "    \n",
    "    def get_mfcc(self): \n",
    "        file_name = '/project/graziul/data/Zone1/2018_08_04/' + self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "              'n_fft': n_fft,\n",
    "              'n_mels': n_mels,\n",
    "              'hop_length': hop_length,\n",
    "              'mel_scale': 'htk',\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.mfcc = mfcc_transform(self.waveform)\n",
    "        return self.mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "input_list = []\n",
    "labels_list = []\n",
    "\n",
    "for idx,key in enumerate(vad_dict):\n",
    "    print(idx)\n",
    "    a = audio_file(key)\n",
    "    a.get_slices(vad_dict)\n",
    "    input_list.append(a.get_mfcc()[:,:,:180409])\n",
    "    a.get_frames()\n",
    "    labels_list.append(a.get_labels()[:180409])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = torch.cat(input_list[:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/project/graziul/ra/anishk/VAD/Source/Data/data1.pt'\n",
    "data = torch.load(f)\n",
    "data = np.transpose(data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 183714, 40])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize LSTM\n",
    "Pytorchâ€™s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 64 \n",
    "n_layers = 3 \n",
    "\n",
    "lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "out, hidden = lstm(data, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_attention(hidden_feature_map):\n",
    "    H_maxtemp = torch.max(hidden_feature_map, 1).values\n",
    "    H_avgtemp = torch.mean(hidden_feature_map, 1)\n",
    "    H_stdtemp = torch.std(hidden_feature_map, 1)\n",
    "    H_concattemp = torch.cat([H_maxtemp[None, :], H_avgtemp[None, :], H_stdtemp[None,:]], dim=0)\n",
    "    return H_concattemp\n",
    "\n",
    "def freq_attention(hidden_feature_map):\n",
    "    H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "    H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "    H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "    H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "    return H_concatfreq \n",
    "\n",
    "def convolve(input,H):\n",
    "    # Define normalization and relu functions for use after first 3 convolutions\n",
    "    norm = nn.BatchNorm1d(64, affine=False, track_running_stats=False)\n",
    "    ReLU = nn.ReLU()\n",
    "\n",
    "    # 1D Convolution; padding of 5 on both sides to account for ndims change\n",
    "    conv1 = nn.Conv1d(3,3, kernel_size=11, padding=5)\n",
    "    output = conv1(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv2 = nn.Conv1d(3,5, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    output = conv2(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv3 = nn.Conv1d(5,5, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    output = conv3(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv4 = nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    H_temp = conv4(input)\n",
    "    # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "    H_temp = H_temp.expand(-1,64,-1)\n",
    "    # Sigmoid activation     \n",
    "    sigmoid = nn.Sigmoid()\n",
    "    input = H_temp\n",
    "    H_temp = sigmoid(input)\n",
    "    H_temp = torch.transpose(H_temp, 1, 2)[0]\n",
    "    # Merge H_temp and H by element wise summation\n",
    "    H_prime = torch.stack((H,H_temp))\n",
    "    H_prime = torch.sum(H_prime,0)\n",
    "    return H_prime\n",
    "\n",
    "#H = out[0] ##H is the \"Hidden feature map\"\n",
    "#input = temp_attention(H)[None,:] ## batch_size, channels, features\n",
    "#output = convolve(input,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(tensor):\n",
    "    input = tensor.detach().numpy()\n",
    "    a = plt.hist(input[0][0], bins = 50)\n",
    "    b = plt.hist(input[0][1], bins = 50)\n",
    "    c = plt.hist(input[0][2], bins = 50)\n",
    "    plt.show()\n",
    "    d = print('max:',np.max(input))\n",
    "    e = print('min:',np.min(input))\n",
    "    range = np.max(input)-np.min(input)\n",
    "    f = print('range:',range)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.9705805\n",
      "min: 0.00013183792\n",
      "range: 0.9704487\n"
     ]
    }
   ],
   "source": [
    "H = out[0] ##H is the \"Hidden feature map\"\n",
    "input = temp_attention(H)[None,:] ## batch_size, channels, features\n",
    "\n",
    "# Define normalization and relu functions for use after first 3 convolutions\n",
    "norm = nn.BatchNorm1d(64, affine=False, track_running_stats=False)\n",
    "ReLU = nn.ReLU()\n",
    "\n",
    "# 1D Convolution; padding of 5 on both sides to account for ndims change\n",
    "conv1 = nn.Conv1d(3,3, kernel_size=11, padding=5)\n",
    "output = conv1(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "conv2 = nn.Conv1d(3,5, kernel_size=11, padding=5)\n",
    "input = output\n",
    "output = conv2(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "\n",
    "conv3 = nn.Conv1d(5,5, kernel_size=11, padding=5)\n",
    "input = output\n",
    "output = conv3(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "conv4 = nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "input = output\n",
    "H_temp = conv4(input)\n",
    "# \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "H_temp = H_temp.expand(-1,64,-1)\n",
    "# Sigmoid activation     \n",
    "sigmoid = nn.Sigmoid()\n",
    "input = H_temp\n",
    "H_temp = sigmoid(input)\n",
    "H_temp = torch.transpose(H_temp, 1, 2)[0]\n",
    "# Merge H_temp and H by element wise summation\n",
    "H_prime = torch.stack((H,H_temp))\n",
    "H_prime = torch.sum(H_prime,0)\n",
    "display(H_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same parameters as LSTM1 except for input dim\n",
    "\n",
    "output = output[0,:,:,:]\n",
    "output = torch.transpose(output,1,2)\n",
    "input_dim = 5\n",
    "hidden_dim = 64 \n",
    "n_layers = 3 \n",
    "\n",
    "lstm2 = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "out, hidden = lstm2(output, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 183714, 64])\n"
     ]
    }
   ],
   "source": [
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(64, 64)\n",
    "linear1_output = linear1(out)\n",
    "\n",
    "linear2 = nn.Linear(64, 1)\n",
    "linear2_output = linear2(linear1_output)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "input = linear2_output\n",
    "output_binary = sigmoid(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), target).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 180409, 40])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.input_dim1 = 40\n",
    "        self.input_dim2 = 64 \n",
    "        self.hidden_dim = 64\n",
    "        self.n_layers = 3\n",
    "        self.batch_size = 2 #184714 makes the code run\n",
    "        #(input is of format batch_size, sequence_length, num_features)\n",
    "        #hidden states should be (num_layers, batch_size, hidden_length)\n",
    "        self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = self.input_dim1, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2 = nn.LSTM(input_size = self.input_dim2, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2_out = None \n",
    "        self.hidden = None\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.convolve1d = nn.Sequential(\n",
    "            nn.Conv1d(3,3, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(3,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "#     def create_rand_hidden1(self):\n",
    "#         self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state1, self.cell_state1)\n",
    "\n",
    "    def temp_attention(self, data):\n",
    "        self.output, hidden = self.lstm1(data, (self.hidden_state1, self.cell_state1))\n",
    "        self.H = self.output\n",
    "        self.H_maxtemp = torch.unsqueeze(torch.max(self.H, -1).values,2)\n",
    "        self.H_avgtemp = torch.unsqueeze(torch.mean(self.H, -1),2)\n",
    "        self.H_stdtemp = torch.unsqueeze(torch.std(self.H, -1),2)\n",
    "        self.H_concattemp = torch.cat([self.H_maxtemp, self.H_avgtemp, self.H_stdtemp], dim=2)\n",
    "        self.H_concattemp = torch.transpose(self.H_concattemp, 1,2)\n",
    "        return self.H_concattemp \n",
    "    \n",
    "    def convolve1(self, data):\n",
    "        self.H_temp = self.convolve1d(self.temp_attention(data))\n",
    "        # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "        self.H_temp = self.H_temp.expand(-1,64,-1)\n",
    "        # Sigmoid activation     \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        self.input = self.H_temp\n",
    "        self.H_temp = sigmoid(self.input)\n",
    "        self.H_temp = torch.transpose(self.H_temp, 1, 2)\n",
    "        # Merge H_temp and H by element wise summation\n",
    "        self.H_prime = torch.stack((self.H,self.H_temp))\n",
    "        self.H_prime = torch.sum(self.H_prime,0)\n",
    "        return self.H_prime\n",
    "        \n",
    "#     def create_rand_hidden2(self):\n",
    "#         self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state2, self.cell_state2)  \n",
    "    \n",
    "#     def freq_attention(hidden_feature_map):\n",
    "#         H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "#         H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "#         H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "#         H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "#         return H_concatfreq \n",
    "\n",
    "    def forward(self, data):\n",
    "        self.input = self.convolve1(data)\n",
    "        self.lstm2_out, self.hidden = self.lstm2(self.input, (self.hidden_state2, self.cell_state2))\n",
    "        self.output = self.output_stack(self.lstm2_out)\n",
    "        return torch.squeeze(self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=1):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=1,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 180409])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_hat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedLSTM()\n",
    "loss_fn = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 180409, 1])\n"
     ]
    }
   ],
   "source": [
    "output_hat = model(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(output_hat, labels[:2]).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa5947f2670>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath = '/project/graziul/ra/ajays/trimmed_threshold_plots/frame_error_rate_52.png'\n",
    "import matplotlib.image as img\n",
    "my_img = img.imread(filepath)\n",
    "plt.imshow(my_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajay_env",
   "language": "python",
   "name": "ajay_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
