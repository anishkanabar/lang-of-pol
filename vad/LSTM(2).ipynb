{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd84c044ab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/project/graziul/data/Zone1/2018_08_04/2018_08_04vad_dict.pkl','rb')\n",
    "vad_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_key = list(vad_dict.keys())[0]\n",
    "vad_dict[test_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a.get_mfcc()\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_audio(datafile, div_size = 29): #Divide the audio clip into bits of 1 minute each\n",
    "#resizes input arrays from (1,feature_length, time) to (div_size,feature_length,time/div_length)\n",
    "    return np.reshape(datafile,[div_size,datafile.shape[1],datafile.shape[2]//div_size])\n",
    "\n",
    "class audio_file():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.vad_slices = None\n",
    "        self.frames = None\n",
    "        self.frames_labels = None\n",
    "        self.mfcc = None\n",
    "        self.n_clips = 30\n",
    "    \n",
    "    def get_slices(self, vad_dict):\n",
    "        self.vad_slices = vad_dict[self.name]['pydub'][-24]['nonsilent_slices']\n",
    "        return self.vad_slices\n",
    "    \n",
    "    def get_frames(self):\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2])\n",
    "        #frames_array = np.zeros(180409)\n",
    "\n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "\n",
    "            for i in range(start,end):\n",
    "                n = math.floor(i/220)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "            \n",
    "            self.frames = frames_array\n",
    "            print(self.frame.shape)\n",
    "            return self.frames\n",
    "        \n",
    "    def get_split_frames(self):\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frame_arr_list = []\n",
    "        for j in range(self.n_clips):\n",
    "            frames_array = np.zeros(self.mfcc.shape[2])\n",
    "            #frames_array = np.zeros(180409)\n",
    "            start_idx = j*self.n_clips*self.sample_rate\n",
    "            end_idx = j*self.n_clips*self.sample_rate\n",
    "            for v in self.vad_slices:\n",
    "                start = math.floor(v[0]*ms_2_sample)\n",
    "                end = math.ceil(v[1]*ms_2_sample)\n",
    "                if(start > start_idx and end < end_idx):\n",
    "                    for i in range(start,end):\n",
    "                        n = math.floor(i/220)\n",
    "                        j = i%220\n",
    "                        if j <= 110:\n",
    "                            frames_array[n-2] += 1\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=111 and j<=220:\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=221 and j<=330:\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                            frames_array[n+1] += 1\n",
    "                        elif j>=331 and j<=440:\n",
    "                            frames_array[n+1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=441:\n",
    "                            frames_array[n+2] += 1\n",
    "                            frames_array[n+1] += 1\n",
    "                            frames_array[n] += 1\n",
    "            frame_arr_list.append(np.expand_dims(frames_array,axis = 0))        \n",
    "        self.frames = np.concatenate(frame_arr_list,axis = 0)\n",
    "        return self.frames\n",
    "    \n",
    "        \n",
    "    def get_labels(self): \n",
    "        self.frames_labels = np.zeros(len(self.frames))\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "    \n",
    "    def get_split_labels(self):\n",
    "        self.frames_labels = np.zeros_like(self.frames)\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "        \n",
    "    def get_mfcc(self): \n",
    "        file_name = '/project/graziul/data/Zone1/2018_08_04/' + self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        print(self.waveform.shape, self.sample_rate)\n",
    "        self.waveform = self.waveform[:,:1800*self.sample_rate] #Clip the file at 1800s\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "              'n_fft': n_fft,\n",
    "              'n_mels': n_mels,\n",
    "              'hop_length': hop_length,\n",
    "              'mel_scale': 'htk',\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.mfcc = mfcc_transform(self.waveform)\n",
    "        return self.mfcc\n",
    "    \n",
    "    def get_split_mfcc(self):\n",
    "        file_name = '/project/graziul/data/Zone1/2018_08_04/' + self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        print(self.waveform.shape, self.sample_rate)\n",
    "        self.waveform = self.waveform[:,:1800*self.sample_rate] #Clip the file at 1800s\n",
    "        n_clips = self.n_clips\n",
    "        mfcc_list = []\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "        mfcc_transform = T.MFCC(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                  'n_fft': n_fft,\n",
    "                  'n_mels': n_mels,\n",
    "                  'hop_length': hop_length,\n",
    "                  'mel_scale': 'htk',\n",
    "                }\n",
    "            )\n",
    "        for i in range(n_clips):\n",
    "            mfcc_list.append(mfcc_transform(self.waveform[:,i*n_clips*self.sample_rate:(i+1)*n_clips*self.sample_rate]))\n",
    "        self.mfcc = torch.cat(mfcc_list)\n",
    "        return self.mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = audio_file(test_key)\n",
    "a.get_slices(vad_dict)\n",
    "test_mfcc = (a.get_split_mfcc())\n",
    "print(test_mfcc.shape)\n",
    "a.get_split_frames()\n",
    "test_labels= (a.get_split_labels())\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 40416944]) 22050\n",
      "1\n",
      "torch.Size([1, 40387392]) 22050\n",
      "2\n",
      "torch.Size([1, 40374144]) 22050\n",
      "3\n",
      "torch.Size([1, 40387392]) 22050\n",
      "4\n",
      "torch.Size([1, 40400640]) 22050\n",
      "5\n",
      "torch.Size([1, 40400640]) 22050\n",
      "6\n",
      "torch.Size([1, 40374144]) 22050\n",
      "7\n",
      "torch.Size([1, 40400640]) 22050\n",
      "8\n",
      "torch.Size([1, 40374144]) 22050\n",
      "9\n",
      "torch.Size([1, 40413888]) 22050\n",
      "10\n",
      "torch.Size([1, 40413888]) 22050\n",
      "11\n",
      "torch.Size([1, 40400640]) 22050\n",
      "12\n",
      "torch.Size([1, 40400640]) 22050\n",
      "13\n",
      "torch.Size([1, 40387392]) 22050\n",
      "14\n",
      "torch.Size([1, 40413888]) 22050\n",
      "15\n",
      "torch.Size([1, 40373568]) 22050\n",
      "16\n",
      "torch.Size([1, 40400640]) 22050\n",
      "17\n",
      "torch.Size([1, 40413888]) 22050\n",
      "18\n",
      "torch.Size([1, 40387392]) 22050\n",
      "19\n",
      "torch.Size([1, 40377276]) 22050\n",
      "20\n",
      "torch.Size([1, 40387392]) 22050\n",
      "21\n",
      "torch.Size([1, 40400640]) 22050\n",
      "22\n",
      "torch.Size([1, 40387968]) 22050\n",
      "23\n",
      "torch.Size([1, 40400640]) 22050\n",
      "24\n",
      "torch.Size([1, 40400640]) 22050\n",
      "25\n",
      "torch.Size([1, 40401216]) 22050\n",
      "26\n",
      "torch.Size([1, 40400640]) 22050\n",
      "27\n",
      "torch.Size([1, 40387392]) 22050\n",
      "28\n",
      "torch.Size([1, 40387392]) 22050\n",
      "29\n",
      "torch.Size([1, 40387392]) 22050\n",
      "30\n",
      "torch.Size([1, 40400640]) 22050\n",
      "31\n",
      "torch.Size([1, 20862916]) 22050\n",
      "32\n",
      "torch.Size([1, 40400640]) 22050\n",
      "33\n",
      "torch.Size([1, 40387392]) 22050\n",
      "34\n",
      "torch.Size([1, 40427136]) 22050\n",
      "torch.Size([1050, 3007, 40])\n",
      "torch.Size([1050, 3007])\n"
     ]
    }
   ],
   "source": [
    "input_list = []\n",
    "labels_list = []\n",
    "\n",
    "for idx,key in enumerate(vad_dict):\n",
    "    print(idx)\n",
    "    a = audio_file(key)\n",
    "    a.get_slices(vad_dict)\n",
    "    input_list.append(a.get_split_mfcc()) #180409\n",
    "    a.get_split_frames()\n",
    "    labels_list.append(a.get_split_labels()) #180409\n",
    "input_list = torch.cat(input_list)\n",
    "input_list = torch.transpose(input_list,1,2)\n",
    "labels_list = torch.from_numpy(np.concatenate(labels_list,axis = 0))\n",
    "print(input_list.size())\n",
    "print(labels_list.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/project/graziul/ra/anishk/VAD/Source/Data/data1.pt'\n",
    "data = torch.load(f)\n",
    "data = np.transpose(data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize LSTM\n",
    "Pytorch’s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 64 \n",
    "n_layers = 3 \n",
    "\n",
    "lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "out, hidden = lstm(data, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_attention(hidden_feature_map):\n",
    "    H_maxtemp = torch.max(hidden_feature_map, 1).values\n",
    "    H_avgtemp = torch.mean(hidden_feature_map, 1)\n",
    "    H_stdtemp = torch.std(hidden_feature_map, 1)\n",
    "    H_concattemp = torch.cat([H_maxtemp[None, :], H_avgtemp[None, :], H_stdtemp[None,:]], dim=0)\n",
    "    return H_concattemp\n",
    "\n",
    "def freq_attention(hidden_feature_map):\n",
    "    H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "    H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "    H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "    H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "    return H_concatfreq \n",
    "\n",
    "def convolve(input,H):\n",
    "    # Define normalization and relu functions for use after first 3 convolutions\n",
    "    norm = nn.BatchNorm1d(64, affine=False, track_running_stats=False)\n",
    "    ReLU = nn.ReLU()\n",
    "\n",
    "    # 1D Convolution; padding of 5 on both sides to account for ndims change\n",
    "    conv1 = nn.Conv1d(3,3, kernel_size=11, padding=5)\n",
    "    output = conv1(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv2 = nn.Conv1d(3,5, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    output = conv2(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv3 = nn.Conv1d(5,5, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    output = conv3(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv4 = nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    H_temp = conv4(input)\n",
    "    # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "    H_temp = H_temp.expand(-1,64,-1)\n",
    "    # Sigmoid activation     \n",
    "    sigmoid = nn.Sigmoid()\n",
    "    input = H_temp\n",
    "    H_temp = sigmoid(input)\n",
    "    H_temp = torch.transpose(H_temp, 1, 2)[0]\n",
    "    # Merge H_temp and H by element wise summation\n",
    "    H_prime = torch.stack((H,H_temp))\n",
    "    H_prime = torch.sum(H_prime,0)\n",
    "    return H_prime\n",
    "\n",
    "#H = out[0] ##H is the \"Hidden feature map\"\n",
    "#input = temp_attention(H)[None,:] ## batch_size, channels, features\n",
    "#output = convolve(input,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(tensor):\n",
    "    input = tensor.detach().numpy()\n",
    "    a = plt.hist(input[0][0], bins = 50)\n",
    "    b = plt.hist(input[0][1], bins = 50)\n",
    "    c = plt.hist(input[0][2], bins = 50)\n",
    "    plt.show()\n",
    "    d = print('max:',np.max(input))\n",
    "    e = print('min:',np.min(input))\n",
    "    range = np.max(input)-np.min(input)\n",
    "    f = print('range:',range)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = out[0] ##H is the \"Hidden feature map\"\n",
    "input = temp_attention(H)[None,:] ## batch_size, channels, features\n",
    "\n",
    "# Define normalization and relu functions for use after first 3 convolutions\n",
    "norm = nn.BatchNorm1d(64, affine=False, track_running_stats=False)\n",
    "ReLU = nn.ReLU()\n",
    "\n",
    "# 1D Convolution; padding of 5 on both sides to account for ndims change\n",
    "conv1 = nn.Conv1d(3,3, kernel_size=11, padding=5)\n",
    "output = conv1(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "conv2 = nn.Conv1d(3,5, kernel_size=11, padding=5)\n",
    "input = output\n",
    "output = conv2(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "\n",
    "conv3 = nn.Conv1d(5,5, kernel_size=11, padding=5)\n",
    "input = output\n",
    "output = conv3(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "conv4 = nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "input = output\n",
    "H_temp = conv4(input)\n",
    "# \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "H_temp = H_temp.expand(-1,64,-1)\n",
    "# Sigmoid activation     \n",
    "sigmoid = nn.Sigmoid()\n",
    "input = H_temp\n",
    "H_temp = sigmoid(input)\n",
    "H_temp = torch.transpose(H_temp, 1, 2)[0]\n",
    "# Merge H_temp and H by element wise summation\n",
    "H_prime = torch.stack((H,H_temp))\n",
    "H_prime = torch.sum(H_prime,0)\n",
    "display(H_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same parameters as LSTM1 except for input dim\n",
    "\n",
    "output = output[0,:,:,:]\n",
    "output = torch.transpose(output,1,2)\n",
    "input_dim = 5\n",
    "hidden_dim = 64 \n",
    "n_layers = 3 \n",
    "\n",
    "lstm2 = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "out, hidden = lstm2(output, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(64, 64)\n",
    "linear1_output = linear1(out)\n",
    "\n",
    "linear2 = nn.Linear(64, 1)\n",
    "linear2_output = linear2(linear1_output)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "input = linear2_output\n",
    "output_binary = sigmoid(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), target).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.input_dim1 = 40\n",
    "        self.input_dim2 = 64 \n",
    "        self.hidden_dim = 64\n",
    "        self.n_layers = 3\n",
    "        self.batch_size = 30\n",
    "        #(input is of format batch_size, sequence_length, num_features)\n",
    "        #hidden states should be (num_layers, batch_size, hidden_length)\n",
    "        self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = self.input_dim1, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2 = nn.LSTM(input_size = self.input_dim2, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2_out = None \n",
    "        self.hidden = None\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.convolve1d = nn.Sequential(\n",
    "            nn.Conv1d(3,3, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(3,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "#     def create_rand_hidden1(self):\n",
    "#         self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state1, self.cell_state1)\n",
    "\n",
    "    def temp_attention(self, data):\n",
    "        H, hidden = self.lstm1(data, (self.hidden_state1, self.cell_state1))\n",
    "        H_maxtemp = torch.unsqueeze(torch.max(H, -1).values,2)\n",
    "        H_avgtemp = torch.unsqueeze(torch.mean(H, -1),2)\n",
    "        H_stdtemp = torch.unsqueeze(torch.std(H, -1),2)\n",
    "        H_concattemp = torch.cat([H_maxtemp, H_avgtemp,H_stdtemp], dim=2)\n",
    "        H_concattemp = torch.transpose(H_concattemp, 1,2)\n",
    "        return H_concattemp,H \n",
    "    \n",
    "    def convolve1(self, data):\n",
    "        H_concattemp,H = self.temp_attention(data)\n",
    "        H_temp = self.convolve1d(H_concattemp)\n",
    "        # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "        H_temp = H_temp.expand(-1,64,-1)\n",
    "        # Sigmoid activation     \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        my_input = H_temp\n",
    "        H_temp = sigmoid(my_input)\n",
    "        H_temp = torch.transpose(H_temp, 1, 2)\n",
    "        # Merge H_temp and H by element wise summation\n",
    "        H_prime = torch.stack((H,H_temp))\n",
    "        H_prime = torch.sum(H_prime,0)\n",
    "        return H_prime\n",
    "        \n",
    "#     def create_rand_hidden2(self):\n",
    "#         self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state2, self.cell_state2)  \n",
    "    \n",
    "#     def freq_attention(hidden_feature_map):\n",
    "#         H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "#         H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "#         H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "#         H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "#         return H_concatfreq \n",
    "\n",
    "    def forward(self, data):\n",
    "        input1 = self.convolve1(data)\n",
    "        lstm2_out, hidden = self.lstm2(input1, (self.hidden_state2, self.cell_state2))\n",
    "        self.output = self.output_stack(lstm2_out)\n",
    "        return torch.squeeze(self.output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=1):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=1,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        #BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        #return torch.mean(BCE_loss)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = torch.tensor(labels_list[:31])\n",
    "labels_list.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedLSTM().to(device)\n",
    "loss_fn = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = input_list[:30]\n",
    "labels_list = labels_list[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1050, 3007, 40])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0.6065, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "1\n",
      "tensor(0.5656, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(0.5277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(0.4938, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(0.4643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(0.4400, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(0.4206, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(0.4056, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(0.3941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(0.3853, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(0.3786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(0.3735, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(0.3694, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(0.3662, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(0.3637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(0.3616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(0.3598, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(0.3584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(0.3572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(0.3562, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(0.3553, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(0.3546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(0.3539, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(0.3533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(0.3528, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(0.3524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(0.3520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(0.3516, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(0.3513, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(0.3510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(0.3508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(0.3506, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(0.3503, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(0.3501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(0.3500, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(0.3498, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(0.3496, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(0.3495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(0.3494, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(0.3493, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(0.3492, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(0.3490, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(0.3489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(0.3489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(0.3488, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(0.3487, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(0.3486, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "47\n",
      "tensor(0.3485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(0.3485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(0.3484, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "training_steps = 50\n",
    "num_samples = input_list.size()[0]//30\n",
    "idx = 0\n",
    "for step in range(training_steps):\n",
    "    input_batch = input_list[idx*30:(idx+1)*30]\n",
    "    labels_batch = labels_list[idx*30:(idx+1)*30]\n",
    "    idx = (idx+1)%num_samples\n",
    "    print(step)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0.9)\n",
    "    optimizer.zero_grad()\n",
    "    output_hat = model(input_batch)\n",
    "    loss = loss_fn(output_hat, labels_batch)\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1050, 30]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/jobs/2641060/ipykernel_4114644/1201269913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultilabel_confusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1050, 30]"
     ]
    }
   ],
   "source": [
    "outputs = (np.round(output_hat.detach().numpy()))\n",
    "labels = (labels_list.detach().numpy())\n",
    "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, classification_report\n",
    "print(accuracy_score(labels,outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/project/graziul/ra/ajays/trimmed_threshold_plots/frame_error_rate_52.png'\n",
    "import matplotlib.image as img\n",
    "my_img = img.imread(filepath)\n",
    "plt.imshow(my_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajay_env",
   "language": "python",
   "name": "ajay_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
