{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1b54029af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filename(filename):\n",
    "    chars_to_remove = ['_','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','.']\n",
    "    my_filename = filename.lower()\n",
    "    for char in chars_to_remove:\n",
    "        my_filename = my_filename.replace(char,\"\")\n",
    "    return my_filename\n",
    "\n",
    "def get_info_from_fname(filename):\n",
    "    year = filename[0:4]\n",
    "    month = filename[4:6]\n",
    "    date = filename[6:8]\n",
    "    sub_file = year + '_' + month + '_' + date\n",
    "    file_loc = sub_file + '/' + filename\n",
    "    return file_loc\n",
    "    \n",
    "def get_whitelisted_filepaths(new_filename_list): #NOT IN USE\n",
    "    zone_paths = '/project/graziul/data/Zone'\n",
    "    file_list = []\n",
    "    for file in new_filename_list:\n",
    "        for i in range(15):\n",
    "            zone_path = zone_paths + str(i) + '/'\n",
    "            #print(zone_path + file)\n",
    "            my_file = zone_path + get_info_from_fname(file)\n",
    "            if(os.path.exists(my_file)):\n",
    "                file_list.append(my_file)\n",
    "    return file_list\n",
    "\n",
    "def is_in_list(my_list,elt):\n",
    "    for my_elt in my_list:\n",
    "        #print(elt, my_elt)\n",
    "        if elt == my_elt:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "datapath = '/project/graziul/data/whitelisted_vad_files.csv'\n",
    "dataframe = pd.read_csv(datapath, header=None)\n",
    "\n",
    "transcripts_path = '/project/graziul/transcripts/transcripts2021_10_27.csv'\n",
    "transcripts_df = pd.read_csv(transcripts_path)\n",
    "df_groups = transcripts_df.groupby(['zone','day','month','year','file'])\n",
    "#clean_transcripts_df_files = [process_filename(transcripts_file) for transcripts_file in list(transcripts_df['file'])]\n",
    "#print(clean_transcripts_df_files)\n",
    "\n",
    "new_filename_list = []\n",
    "chars_to_remove = ['_','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','.']\n",
    "for idx,elt in enumerate(list(dataframe[0])):\n",
    "    my_elt = process_filename(elt)\n",
    "    new_filename_list.append(my_elt[:-1] + '.mp3')\n",
    "    #new_filename_list.append(my_elt[:-1])\n",
    "new_filename_list = list(set(new_filename_list))\n",
    "\n",
    "to_ms = 1000\n",
    "fname_list = []\n",
    "transcripts_list = []\n",
    "for state,frame in df_groups: \n",
    "    #print(frame)\n",
    "    info_list = list(state)\n",
    "    \n",
    "    zone = info_list[0]\n",
    "    day = info_list[1]\n",
    "    month = info_list[2]\n",
    "    year = info_list[3]\n",
    "    if(len(str(day)) > 1):\n",
    "        str_day = str(day)\n",
    "    else:\n",
    "        str_day = '0' + str(day)\n",
    "    date = str(year) + '_0' + str(month) + '_' + str_day\n",
    "    filename = process_filename(info_list[4]) + '.mp3'\n",
    "    fpath = '/project/graziul/data/' + zone + '/' + date + '/' + filename\n",
    "    print(list(state))\n",
    "    print(fpath)\n",
    "    #print(filename)\n",
    "    if(is_in_list(new_filename_list, filename) == 1):\n",
    "        #print(fpath)\n",
    "        if(os.path.exists(fpath)):\n",
    "            #print(frame)\n",
    "            if(is_in_list(fname_list,fpath) == 0):\n",
    "                print(list(set(list(frame['transcriber']))))\n",
    "                #print(frame.head())\n",
    "                start_times = list(frame['start'])\n",
    "                end_times = list(frame['end'])\n",
    "                start_samples = [(int)(to_ms*start_time) for start_time in start_times]  #Convert to milliseconds\n",
    "                end_samples = [(int)(to_ms*end_time) for end_time in end_times]  #Convert to milliseconds\n",
    "                #transcripts = list(zip(start_times,end_times))\n",
    "                transcripts = list(zip(start_samples,end_samples))\n",
    "                transcripts_list.append(transcripts)\n",
    "                fname_list.append(fpath)\n",
    "            else:\n",
    "                pass\n",
    "                #print(fpath)\n",
    "#fname_list = list(set(fname_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vad_dict = {}\n",
    "for i,fname in enumerate(fname_list):\n",
    "    new_vad_dict[fname] = {'nonsilent_slices': transcripts_list[i], 'units':'milliseconds'}\n",
    "\n",
    "pkl_path = '/project/graziul/ra/ajays/whitelisted_vad_dict.pkl' \n",
    "file = open(pkl_path,'wb')\n",
    "pickle.dump(new_vad_dict,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = '/project/graziul/ra/ajays/whitelisted_vad_dict.pkl' \n",
    "#pkl_path = '/project/graziul/data/Zone1/2018_08_04/2018_08_04vad_dict.pkl'\n",
    "file = open(pkl_path,'rb')\n",
    "vad_dict = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_audio(datafile, div_size = 30): #Divide the audio clip into bits of 1 minute each\n",
    "#resizes input arrays from (1,feature_length, time) to (div_size,feature_length,time/div_length)\n",
    "    return np.reshape(datafile,[div_size,datafile.shape[1],datafile.shape[2]//div_size])\n",
    "\n",
    "class audio_file():\n",
    "    def __init__(self, name,new_flag = 1):\n",
    "        self.name = name\n",
    "        self.vad_slices = None\n",
    "        self.frames = None\n",
    "        self.frames_labels = None\n",
    "        self.mfcc = None\n",
    "        self.n_clips = 30\n",
    "        self.flag = new_flag\n",
    "    \n",
    "    def get_slices(self, vad_dict):\n",
    "        if self.flag == 1:\n",
    "            self.vad_slices = vad_dict[self.name]['nonsilent_slices']\n",
    "        else:\n",
    "            self.vad_slices = vad_dict[self.name]['pydub'][-24]['nonsilent_slices']\n",
    "        return self.vad_slices\n",
    "    \n",
    "    def get_frames(self):\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2])\n",
    "\n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "            #print(v)\n",
    "            for i in range(start,end):\n",
    "                n = math.floor(i/220)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "            \n",
    "        self.frames = frames_array\n",
    "        return self.frames\n",
    "        \n",
    "    def get_split_frames(self):\n",
    "        '''ms_2_sample = self.sample_rate/1000\n",
    "        frame_arr_list = []\n",
    "        for j in range(self.n_clips):\n",
    "            frames_array = np.zeros(self.mfcc.shape[2])\n",
    "            #frames_array = np.zeros(180409)\n",
    "            self.clip_size = self.mfcc.shape[2]\n",
    "            start_idx = j*self.clip_size\n",
    "            end_idx = j*self.clip_size\n",
    "            print(start_idx, end_idx)\n",
    "            for v in self.vad_slices:\n",
    "                start = math.floor(v[0]*ms_2_sample)\n",
    "                end = math.ceil(v[1]*ms_2_sample)\n",
    "                if(start >= start_idx and end <= end_idx):\n",
    "                    for i in range(start,end):\n",
    "                        n = math.floor(i/220)\n",
    "                        j = i%220\n",
    "                        if j <= 110:\n",
    "                            frames_array[n-2] += 1\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=111 and j<=220:\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=221 and j<=330:\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                            frames_array[n+1] += 1\n",
    "                        elif j>=331 and j<=440:\n",
    "                            frames_array[n+1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=441:\n",
    "                            frames_array[n+2] += 1\n",
    "                            frames_array[n+1] += 1\n",
    "                            frames_array[n] += 1\n",
    "            frame_arr_list.append(np.expand_dims(frames_array,axis = 0))        \n",
    "        self.frames = np.concatenate(frame_arr_list,axis = 0)\n",
    "        return self.frames'''\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2]*self.n_clips)\n",
    "        print(frames_array.shape)\n",
    "\n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "            for i in range(start,end):\n",
    "                n = min(math.floor(i/220),len(frames_array)-1)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "        \n",
    "        self.clip_size = self.mfcc.shape[2]\n",
    "        frame_arr_list = []\n",
    "        for j in range(self.n_clips):\n",
    "            frame_arr_list.append(np.expand_dims(frames_array[j*self.clip_size:(j+1)*self.clip_size],axis=0))\n",
    "        self.frames = np.concatenate(frame_arr_list,axis=0)\n",
    "        return self.frames\n",
    "    \n",
    "        \n",
    "    def get_labels(self): \n",
    "        self.frames_labels = np.zeros(len(self.frames))\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "    \n",
    "    def get_split_labels(self):\n",
    "        self.frames_labels = np.zeros_like(self.frames)\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "        \n",
    "    def get_mfcc(self): \n",
    "        if self.flag == 0:\n",
    "            file_name = '/project/graziul/data/Zone1/2018_08_04/' + self.name\n",
    "        else:\n",
    "            file_name = self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        self.waveform = self.waveform[:,:1800*self.sample_rate] #Clip the file at 1800s\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "              'n_fft': n_fft,\n",
    "              'n_mels': n_mels,\n",
    "              'hop_length': hop_length,\n",
    "              'mel_scale': 'htk',\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.mfcc = mfcc_transform(self.waveform)\n",
    "        return self.mfcc\n",
    "    \n",
    "    def get_split_mfcc(self):\n",
    "        if self.flag == 1:\n",
    "            file_name = self.name\n",
    "        else:\n",
    "            file_name = '/project/graziul/data/Zone1/2018_08_04/' + self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        self.waveform = self.waveform[:,:1800*self.sample_rate] #Clip the file at 1800s\n",
    "        clip_size = math.floor(self.waveform.shape[1]/self.n_clips)\n",
    "        n_clips = self.n_clips\n",
    "        mfcc_list = []\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "        mfcc_transform = T.MFCC(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                  'n_fft': n_fft,\n",
    "                  'n_mels': n_mels,\n",
    "                  'hop_length': hop_length,\n",
    "                  'mel_scale': 'htk',\n",
    "                }\n",
    "            )\n",
    "        for i in range(n_clips):\n",
    "            mfcc_list.append(mfcc_transform(self.waveform[:,i*clip_size:(i+1)*clip_size]))\n",
    "        self.mfcc = torch.cat(mfcc_list)\n",
    "        return self.mfcc\n",
    "    \n",
    "    def plot_waveform_with_labels(self,i,clip_size):\n",
    "        plt.figure(figsize=(14,5))\n",
    "        fig,(ax1,ax2) = plt.subplots(2,1)\n",
    "        librosa.display.waveshow(self.waveform.squeeze().numpy()[i*clip_size:(i+1)*clip_size],self.sample_rate,ax = ax1)\n",
    "        ax2.plot(self.frames_labels[i])\n",
    "        plt.show()\n",
    "        return    \n",
    "    \n",
    "    def get_plots(self): \n",
    "        clip_size = math.floor(1800*self.sample_rate/self.n_clips)\n",
    "        for i in range(self.n_clips):\n",
    "            print(i)\n",
    "            self.plot_waveform_with_labels(i,clip_size)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(180420,)\n",
      "1\n",
      "(180420,)\n",
      "2\n",
      "(180420,)\n",
      "3\n",
      "(180420,)\n",
      "4\n",
      "(180420,)\n",
      "5\n",
      "(180420,)\n",
      "6\n",
      "(180420,)\n",
      "7\n",
      "(180420,)\n",
      "8\n",
      "(180420,)\n",
      "9\n",
      "(180420,)\n",
      "10\n",
      "(180420,)\n",
      "11\n",
      "(180420,)\n",
      "12\n",
      "(180420,)\n",
      "13\n",
      "(180420,)\n",
      "14\n",
      "(180420,)\n",
      "15\n",
      "(180420,)\n",
      "16\n",
      "(180420,)\n",
      "17\n",
      "(180420,)\n",
      "18\n",
      "(180420,)\n",
      "19\n",
      "(180420,)\n",
      "20\n",
      "torch.Size([600, 6014, 40])\n",
      "torch.Size([600, 6014])\n"
     ]
    }
   ],
   "source": [
    "input_list = []\n",
    "labels_list = []\n",
    "\n",
    "for idx,key in enumerate(vad_dict):\n",
    "    print(idx)\n",
    "    if(idx == 20):\n",
    "        break\n",
    "    a = audio_file(key)\n",
    "    a.get_slices(vad_dict)\n",
    "    input_list.append(a.get_split_mfcc()) \n",
    "    a.get_split_frames()\n",
    "    labels_list.append(a.get_split_labels()) \n",
    "    #a.get_plots()\n",
    "input_list = torch.cat(input_list)\n",
    "input_list = torch.transpose(input_list,1,2)\n",
    "labels_list = torch.from_numpy(np.concatenate(labels_list,axis = 0)).float()\n",
    "print(input_list.size())\n",
    "print(labels_list.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/project/graziul/ra/anishk/VAD/Source/Data/data1.pt'\n",
    "data = torch.load(f)\n",
    "data = np.transpose(data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize LSTM\n",
    "Pytorchâ€™s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.input_dim1 = 40\n",
    "        self.input_dim2 = 64 \n",
    "        self.hidden_dim = 64\n",
    "        self.n_layers = 3\n",
    "        self.batch_size = 5\n",
    "        #(input is of format batch_size, sequence_length, num_features)\n",
    "        #hidden states should be (num_layers, batch_size, hidden_length)\n",
    "        self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = self.input_dim1, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2 = nn.LSTM(input_size = self.input_dim2, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2_out = None \n",
    "        self.hidden = None\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.convolve1d = nn.Sequential(\n",
    "            nn.Conv1d(3,3, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(3,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def create_rand_hidden1(self):\n",
    "#         self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state1, self.cell_state1)\n",
    "\n",
    "    def lstm1_func(self,data, unroll = 0):\n",
    "        if(unroll ==0):\n",
    "            H, hidden = self.lstm1(data, (self.hidden_state1, self.cell_state1)) \n",
    "        else:\n",
    "            hn = self.hidden_state1\n",
    "            cn = self.cell_state1\n",
    "            H = []\n",
    "            for idx in range(data.size()[1]):\n",
    "                output, (hn,cn) = self.lstm1(torch.unsqueeze(data[:,idx,:],1), (hn,cn))\n",
    "                H.append(output)\n",
    "            H = torch.cat(H,dim=0)\n",
    "            H = output\n",
    "            hidden = (hn,cn)\n",
    "        return H,hidden\n",
    "\n",
    "    def temp_attention(self, data):\n",
    "        H,hidden = self.lstm1_func(data, 0)\n",
    "        H_maxtemp = torch.unsqueeze(torch.max(H, -1).values,2)\n",
    "        H_avgtemp = torch.unsqueeze(torch.mean(H, -1),2)\n",
    "        H_stdtemp = torch.unsqueeze(torch.std(H, -1),2)\n",
    "        H_concattemp = torch.cat([H_maxtemp, H_avgtemp,H_stdtemp], dim=2)\n",
    "        H_concattemp = torch.transpose(H_concattemp, 1,2)\n",
    "        return H_concattemp,H \n",
    "    \n",
    "    def convolve1(self, data):\n",
    "        H_concattemp,H = self.temp_attention(data)\n",
    "        H_temp = self.convolve1d(H_concattemp)\n",
    "        # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "        H_temp = H_temp.expand(-1,64,-1)\n",
    "        # Sigmoid activation     \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        my_input = H_temp\n",
    "        H_temp = sigmoid(my_input)\n",
    "        H_temp = torch.transpose(H_temp, 1, 2)\n",
    "        # Merge H_temp and H by element wise summation\n",
    "        H_prime = torch.stack((H,H_temp))\n",
    "        H_prime = torch.sum(H_prime,0)\n",
    "        return H_prime\n",
    "        \n",
    "#     def create_rand_hidden2(self):\n",
    "#         self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state2, self.cell_state2)  \n",
    "    \n",
    "#     def freq_attention(hidden_feature_map):\n",
    "#         H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "#         H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "#         H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "#         H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "#         return H_concatfreq \n",
    "\n",
    "    def lstm2_func(self,input1, unroll =0):\n",
    "        if(unroll==0):\n",
    "            lstm2_out, hidden = self.lstm2(input1, (self.hidden_state2, self.cell_state2))\n",
    "        else:\n",
    "            hn = self.hidden_state2\n",
    "            cn = self.cell_state2\n",
    "            lstm2_out = []\n",
    "            for idx in range(input1.size()[1]):\n",
    "                output, (hn,cn) = self.lstm2(torch.unsqueeze(input1[:,idx,:],1), (hn,cn))\n",
    "                lstm2_out.append(output)\n",
    "            lstm2_out = torch.cat(lstm2_out,dim=0)\n",
    "            hidden = (hn,cn)\n",
    "        return lstm2_out,hidden\n",
    "\n",
    "    def forward(self, data):\n",
    "        input1 = self.convolve1(data)\n",
    "        lstm2_out, hidden = self.lstm2_func(input1)\n",
    "        self.output = self.output_stack(lstm2_out)\n",
    "        print(self.output)\n",
    "        self.output = torch.squeeze(self.output)\n",
    "        return self.output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.input_dim1 = 40\n",
    "        self.input_dim2 = 64 \n",
    "        self.hidden_dim = 64\n",
    "        self.n_layers = 3\n",
    "        self.batch_size = 5\n",
    "        #(input is of format batch_size, sequence_length, num_features)\n",
    "        #hidden states should be (num_layers, batch_size, hidden_length)\n",
    "        self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = self.input_dim1, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2 = nn.LSTM(input_size = self.input_dim2, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2_out = None \n",
    "        self.hidden = None\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.convolve1d = nn.Sequential(\n",
    "            nn.Conv1d(3,3, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(3,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        out1,_ = self.lstm1(data,(self.hidden_state1,self.cell_state1))\n",
    "        out2 = self.sigmoid(self.output_stack(out1))\n",
    "        return torch.squeeze(out2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=1):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha])\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=1,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        #print(inputs)\n",
    "        #print(targets)\n",
    "        BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        return torch.mean(BCE_loss)\n",
    "        #ce_loss = F.binary_cross_entropy(inputs, targets,reduction=self.reduction,weight=self.weight)\n",
    "        #pt = torch.exp(-ce_loss)\n",
    "        #focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        #return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = StackedLSTM().to(device)\n",
    "model = ToyModel()\n",
    "loss_fn = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_steps = 100\n",
    "batch_size = model.batch_size\n",
    "num_samples = input_list.size()[0]//batch_size\n",
    "idx = 0\n",
    "flag = 0\n",
    "for step in range(training_steps):\n",
    "    input_batch = input_list[idx*batch_size:(idx+1)*batch_size]\n",
    "    labels_batch = labels_list[idx*batch_size:(idx+1)*batch_size]\n",
    "    idx = (idx+1)%num_samples\n",
    "    print(step)\n",
    "    optimizer.zero_grad()\n",
    "    output_hat = model(input_batch)\n",
    "    #print(output_hat)\n",
    "    loss = loss_fn(output_hat, labels_batch)\n",
    "    loss.backward()\n",
    "    #for param in model.parameters():\n",
    "    #    print(param.grad)\n",
    "    print(loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(input_list, label_list):\n",
    "    output_list = []\n",
    "    idx = 0\n",
    "    num_samples = labels_list.size()[0]//batch_size\n",
    "    with torch.no_grad():\n",
    "        while(idx < num_samples):\n",
    "            print(idx)\n",
    "            input_batch = input_list[idx*batch_size:(idx+1)*batch_size]\n",
    "            labels_batch = labels_list[idx*batch_size:(idx+1)*batch_size]\n",
    "            idx = idx+1\n",
    "            output_hat = model(input_batch)\n",
    "            #print(output_hat)\n",
    "            #for param in model.parameters():\n",
    "            #    print(param.grad)\n",
    "            output_list.append(output_hat)\n",
    "        output_list = torch.cat(output_list, dim = 0)\n",
    "        return output_list\n",
    "\n",
    "def get_frame_error_rate(output_hat, labels):\n",
    "    num_samples = labels.size()[0]\n",
    "    fer_arr = []\n",
    "    for i in range(num_samples):\n",
    "        curr_output = output_hat[i]\n",
    "        curr_label = labels[i]\n",
    "        fer_arr.append(torch.mean(torch.add(curr_output,curr_label)%2).data*100)\n",
    "    return fer_arr\n",
    "\n",
    "def test_frame_error_rate(output_hat, labels):\n",
    "    num_samples = labels.size()[0]\n",
    "    s_length = labels.size()[1]\n",
    "    fer_arr = []\n",
    "    sum = 0\n",
    "    for i in range(num_samples):\n",
    "        curr_output = output_hat[i]\n",
    "        curr_label = labels[i]\n",
    "        for j in range(s_length):\n",
    "            if curr_output[j] == curr_label[j]:\n",
    "                pass\n",
    "            else:\n",
    "                sum = sum+1\n",
    "        fer_arr.append(torch.mean(torch.add(curr_output,curr_label)%2)*100)\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor(0.6916, grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(0.5399, grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(0.1564, grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(0.1513, grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(0.0498, grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(0.4664, grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(0.2077, grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(0.2552, grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(0.1588, grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(0.1295, grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(0.2332, grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(0.0520, grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(0.0109, grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(0.0139, grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(0.0138, grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(0.0106, grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(0.0063, grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(0.0034, grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(0.0018, grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(0.1188, grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(0.0430, grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(0.0039, grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(0.1785, grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(0.4480, grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(0.1834, grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(0.1754, grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(0.0614, grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(0.1343, grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(0.1385, grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(0.0274, grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(0.0464, grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(0.0127, grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(0.1681, grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(0.1092, grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(0.4534, grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(0.5277, grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(0.1432, grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(0.1377, grad_fn=<MeanBackward0>)\n",
      "47\n",
      "tensor(0.1916, grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(0.1579, grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(0.2949, grad_fn=<MeanBackward0>)\n",
      "50\n",
      "tensor(0.3629, grad_fn=<MeanBackward0>)\n",
      "51\n",
      "tensor(0.0501, grad_fn=<MeanBackward0>)\n",
      "52\n",
      "tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "53\n",
      "tensor(0.0451, grad_fn=<MeanBackward0>)\n",
      "54\n",
      "tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "55\n",
      "tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "56\n",
      "tensor(0.2601, grad_fn=<MeanBackward0>)\n",
      "57\n",
      "tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "58\n",
      "tensor(0.1163, grad_fn=<MeanBackward0>)\n",
      "59\n",
      "tensor(0.0624, grad_fn=<MeanBackward0>)\n",
      "60\n",
      "tensor(0.1429, grad_fn=<MeanBackward0>)\n",
      "61\n",
      "tensor(0.2313, grad_fn=<MeanBackward0>)\n",
      "62\n",
      "tensor(0.1803, grad_fn=<MeanBackward0>)\n",
      "63\n",
      "tensor(0.2145, grad_fn=<MeanBackward0>)\n",
      "64\n",
      "tensor(0.3610, grad_fn=<MeanBackward0>)\n",
      "65\n",
      "tensor(0.1492, grad_fn=<MeanBackward0>)\n",
      "66\n",
      "tensor(0.1593, grad_fn=<MeanBackward0>)\n",
      "67\n",
      "tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "68\n",
      "tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "69\n",
      "tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "70\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "71\n",
      "tensor(0.1477, grad_fn=<MeanBackward0>)\n",
      "72\n",
      "tensor(0.0309, grad_fn=<MeanBackward0>)\n",
      "73\n",
      "tensor(0.2653, grad_fn=<MeanBackward0>)\n",
      "74\n",
      "tensor(0.1856, grad_fn=<MeanBackward0>)\n",
      "75\n",
      "tensor(0.1505, grad_fn=<MeanBackward0>)\n",
      "76\n",
      "tensor(0.1137, grad_fn=<MeanBackward0>)\n",
      "77\n",
      "tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "78\n",
      "tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "79\n",
      "tensor(0.0482, grad_fn=<MeanBackward0>)\n",
      "80\n",
      "tensor(0.1543, grad_fn=<MeanBackward0>)\n",
      "81\n",
      "tensor(0.1139, grad_fn=<MeanBackward0>)\n",
      "82\n",
      "tensor(0.1784, grad_fn=<MeanBackward0>)\n",
      "83\n",
      "tensor(0.1677, grad_fn=<MeanBackward0>)\n",
      "84\n",
      "tensor(0.1844, grad_fn=<MeanBackward0>)\n",
      "85\n",
      "tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "86\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "87\n",
      "tensor(0.1285, grad_fn=<MeanBackward0>)\n",
      "88\n",
      "tensor(0.4353, grad_fn=<MeanBackward0>)\n",
      "89\n",
      "tensor(0.0481, grad_fn=<MeanBackward0>)\n",
      "90\n",
      "tensor(0.1102, grad_fn=<MeanBackward0>)\n",
      "91\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "92\n",
      "tensor(0.0302, grad_fn=<MeanBackward0>)\n",
      "93\n",
      "tensor(0.1639, grad_fn=<MeanBackward0>)\n",
      "94\n",
      "tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "95\n",
      "tensor(0.1562, grad_fn=<MeanBackward0>)\n",
      "96\n",
      "tensor(0.2226, grad_fn=<MeanBackward0>)\n",
      "97\n",
      "tensor(0.1236, grad_fn=<MeanBackward0>)\n",
      "98\n",
      "tensor(0.3065, grad_fn=<MeanBackward0>)\n",
      "99\n",
      "tensor(0.2389, grad_fn=<MeanBackward0>)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (3, 0, 64), got [3, 5, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/jobs/2695487/ipykernel_2967383/1269367539.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_frame_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/jobs/2695487/ipykernel_2967383/3467059897.py\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(input_list, label_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m#print(output_hat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#for param in model.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/jobs/2695487/ipykernel_2967383/2993411170.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_state1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    631\u001b[0m                            ):\n\u001b[1;32m    632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[1;32m    634\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    635\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
      "\u001b[0;32m~/.conda/envs/ajay_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    224\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (3, 0, 64), got [3, 5, 64]"
     ]
    }
   ],
   "source": [
    "model = ToyModel()\n",
    "#model = nn.DataParallel(model)\n",
    "training_steps = 100\n",
    "batch_size = model.batch_size\n",
    "num_samples = input_list.size()[0]//batch_size\n",
    "val_size = 30\n",
    "flag = 0\n",
    "preds_file = open('/project/graziul/ra/ajays/toy_model_predictions.txt','w')\n",
    "preds_file.truncate(0)\n",
    "for val_index in range(num_samples):\n",
    "    #model = StackedLSTM().to(device)\n",
    "    #model = ToyModel().to(device)\n",
    "    #model = nn.DataParallel(model)\n",
    "    loss_fn = FocalLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    test_data = input_list[val_index*val_size:(val_index+1)*val_size] \n",
    "    test_labels = labels_list[val_index*val_size:(val_index+1)*val_size]\n",
    "    train_data = torch.cat([input_list[:val_index*val_size], input_list[(val_index+1)*val_size:]], dim = 0)\n",
    "    train_labels = torch.cat([labels_list[:val_index*val_size], labels_list[(val_index+1)*val_size:]], dim = 0)\n",
    "    print(val_index)\n",
    "    idx = 0\n",
    "    for step in range(training_steps):\n",
    "        if(idx != val_index):\n",
    "            input_batch = train_data[idx*batch_size:(idx+1)*batch_size]\n",
    "            labels_batch = train_labels[idx*batch_size:(idx+1)*batch_size]\n",
    "            print(step)\n",
    "            optimizer.zero_grad()\n",
    "            output_hat = model(input_batch)\n",
    "            #print(output_hat)\n",
    "            loss = loss_fn(output_hat, labels_batch)\n",
    "            loss.backward()\n",
    "            #for param in model.parameters():\n",
    "            #    print(param.grad)\n",
    "            print(loss)\n",
    "            optimizer.step()\n",
    "        idx = (idx+1)%num_samples\n",
    "    preds = get_predictions(test_data, test_labels)\n",
    "    fer = str(get_frame_error_rate(preds,test_labels)) + \"\\n\"\n",
    "    print(fer)\n",
    "    preds_file.write(fer)\n",
    "preds_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = (np.round(output_hat.detach().numpy()))\n",
    "labels = (labels_list.detach().numpy())\n",
    "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, classification_report\n",
    "print(accuracy_score(labels,outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(torch.round(output_hat).detach().numpy(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(input_list, label_list):\n",
    "    output_list = []\n",
    "    idx = 0\n",
    "    num_samples = labels_list.size()[0]//batch_size\n",
    "    with torch.no_grad():\n",
    "        while(idx < num_samples):\n",
    "            print(idx)\n",
    "            input_batch = input_list[idx*batch_size:(idx+1)*batch_size]\n",
    "            labels_batch = labels_list[idx*batch_size:(idx+1)*batch_size]\n",
    "            idx = idx+1\n",
    "            output_hat = model(input_batch)\n",
    "            #print(output_hat)\n",
    "            #for param in model.parameters():\n",
    "            #    print(param.grad)\n",
    "            output_list.append(output_hat)\n",
    "        output_list = torch.cat(output_list, dim = 0)\n",
    "        return output_list\n",
    "    \n",
    "\n",
    "    \n",
    "output_list = get_predictions(input_list,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_error_rate(output_hat, labels):\n",
    "    num_samples = labels.size()[0]\n",
    "    fer_arr = []\n",
    "    for i in range(num_samples):\n",
    "        curr_output = output_hat[i]\n",
    "        curr_label = labels[i]\n",
    "        fer_arr.append(torch.mean(torch.add(curr_output,curr_label)%2).data*100)\n",
    "    return fer_arr\n",
    "\n",
    "def test_frame_error_rate(output_hat, labels):\n",
    "    num_samples = labels.size()[0]\n",
    "    s_length = labels.size()[1]\n",
    "    fer_arr = []\n",
    "    sum = 0\n",
    "    for i in range(num_samples):\n",
    "        curr_output = output_hat[i]\n",
    "        curr_label = labels[i]\n",
    "        for j in range(s_length):\n",
    "            if curr_output[j] == curr_label[j]:\n",
    "                pass\n",
    "            else:\n",
    "                sum = sum+1\n",
    "        fer_arr.append(torch.mean(torch.add(curr_output,curr_label)%2)*100)\n",
    "    return sum\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Frame error Rate :\" + str(get_frame_error_rate(torch.round(output_list),labels_list)))\n",
    "    print(test_frame_error_rate(output_list, labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.round(output_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outputs(input_list, labels_list, output_hat, sample_rate = 22050):\n",
    "    num_samples = input_list.size()[0]\n",
    "    diff_labels = labels_list - output_hat\n",
    "    for i in range(num_samples):\n",
    "        print(i)\n",
    "        plt.figure(figsize=(14,5))\n",
    "        fig,(ax1,ax2) = plt.subplots(2,1)\n",
    "        librosa.display.waveshow(input_list[i].numpy(),sample_rate,ax = ax1)\n",
    "        ax2.plot(diff_labels[i].numpy())\n",
    "        plt.show()\n",
    "    return    \n",
    "    \n",
    "    \n",
    "with torch.no_grad():\n",
    "    plot_outputs(input_list,labels_list,output_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/project/graziul/ra/ajays/trimmed_threshold_plots/frame_error_rate_52.png'\n",
    "import matplotlib.image as img\n",
    "my_img = img.imread(filepath)\n",
    "plt.imshow(my_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajay_env",
   "language": "python",
   "name": "ajay_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
