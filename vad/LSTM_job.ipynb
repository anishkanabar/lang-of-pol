{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "\n",
    "torch.manual_seed(1)\n",
    "def process_filename(filename):\n",
    "    chars_to_remove = ['_','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','.']\n",
    "    my_filename = filename.lower()\n",
    "    for char in chars_to_remove:\n",
    "        my_filename = my_filename.replace(char,\"\")\n",
    "    return my_filename\n",
    "\n",
    "def get_info_from_fname(filename):\n",
    "    year = filename[0:4]\n",
    "    month = filename[4:6]\n",
    "    date = filename[6:8]\n",
    "    sub_file = year + '_' + month + '_' + date\n",
    "    file_loc = sub_file + '/' + filename\n",
    "    return file_loc\n",
    "    \n",
    "def get_whitelisted_filepaths(new_filename_list): #NOT IN USE\n",
    "    zone_paths = '/project/graziul/data/Zone'\n",
    "    file_list = []\n",
    "    for file in new_filename_list:\n",
    "        for i in range(15):\n",
    "            zone_path = zone_paths + str(i) + '/'\n",
    "            #print(zone_path + file)\n",
    "            my_file = zone_path + get_info_from_fname(file)\n",
    "            if(os.path.exists(my_file)):\n",
    "                file_list.append(my_file)\n",
    "    return file_list\n",
    "\n",
    "def is_in_list(my_list,elt):\n",
    "    for my_elt in my_list:\n",
    "        #print(elt, my_elt)\n",
    "        if elt == my_elt:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def divide_audio(datafile, div_size = 30): #Divide the audio clip into bits of 1 minute each\n",
    "#resizes input arrays from (1,feature_length, time) to (div_size,feature_length,time/div_length)\n",
    "    return np.reshape(datafile,[div_size,datafile.shape[1],datafile.shape[2]//div_size])\n",
    "\n",
    "class audio_file():\n",
    "    def __init__(self, name,new_flag = 1):\n",
    "        self.name = name\n",
    "        self.vad_slices = None\n",
    "        self.frames = None\n",
    "        self.frames_labels = None\n",
    "        self.mfcc = None\n",
    "        self.n_clips = 30\n",
    "        self.flag = new_flag\n",
    "    \n",
    "    def get_slices(self, vad_dict):\n",
    "        if self.flag == 1:\n",
    "            self.vad_slices = vad_dict[self.name]['nonsilent_slices']\n",
    "        else:\n",
    "            self.vad_slices = vad_dict[self.name]['pydub'][-24]['nonsilent_slices']\n",
    "        return self.vad_slices\n",
    "    \n",
    "    def get_frames(self):\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2])\n",
    "\n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "            #print(v)\n",
    "            for i in range(start,end):\n",
    "                n = math.floor(i/220)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "            \n",
    "        self.frames = frames_array\n",
    "        return self.frames\n",
    "        \n",
    "    def get_split_frames(self):\n",
    "        '''ms_2_sample = self.sample_rate/1000\n",
    "        frame_arr_list = []\n",
    "        for j in range(self.n_clips):\n",
    "            frames_array = np.zeros(self.mfcc.shape[2])\n",
    "            #frames_array = np.zeros(180409)\n",
    "            self.clip_size = self.mfcc.shape[2]\n",
    "            start_idx = j*self.clip_size\n",
    "            end_idx = j*self.clip_size\n",
    "            print(start_idx, end_idx)\n",
    "            for v in self.vad_slices:\n",
    "                start = math.floor(v[0]*ms_2_sample)\n",
    "                end = math.ceil(v[1]*ms_2_sample)\n",
    "                if(start >= start_idx and end <= end_idx):\n",
    "                    for i in range(start,end):\n",
    "                        n = math.floor(i/220)\n",
    "                        j = i%220\n",
    "                        if j <= 110:\n",
    "                            frames_array[n-2] += 1\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=111 and j<=220:\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=221 and j<=330:\n",
    "                            frames_array[n-1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                            frames_array[n+1] += 1\n",
    "                        elif j>=331 and j<=440:\n",
    "                            frames_array[n+1] += 1\n",
    "                            frames_array[n] += 1\n",
    "                        elif j>=441:\n",
    "                            frames_array[n+2] += 1\n",
    "                            frames_array[n+1] += 1\n",
    "                            frames_array[n] += 1\n",
    "            frame_arr_list.append(np.expand_dims(frames_array,axis = 0))        \n",
    "        self.frames = np.concatenate(frame_arr_list,axis = 0)\n",
    "        return self.frames'''\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2]*self.n_clips)\n",
    "        print(frames_array.shape)\n",
    "\n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "            for i in range(start,end):\n",
    "                n = min(math.floor(i/220),len(frames_array)-1)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "        \n",
    "        self.clip_size = self.mfcc.shape[2]\n",
    "        frame_arr_list = []\n",
    "        for j in range(self.n_clips):\n",
    "            frame_arr_list.append(np.expand_dims(frames_array[j*self.clip_size:(j+1)*self.clip_size],axis=0))\n",
    "        self.frames = np.concatenate(frame_arr_list,axis=0)\n",
    "        return self.frames\n",
    "    \n",
    "        \n",
    "    def get_labels(self): \n",
    "        self.frames_labels = np.zeros(len(self.frames))\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "    \n",
    "    def get_split_labels(self):\n",
    "        self.frames_labels = np.zeros_like(self.frames)\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "        \n",
    "    def get_mfcc(self): \n",
    "        if self.flag == 0:\n",
    "            file_name = '/project/graziul/data/Zone1/2018_08_04/' + self.name\n",
    "        else:\n",
    "            file_name = self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        self.waveform = self.waveform[:,:1800*self.sample_rate] #Clip the file at 1800s\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "              'n_fft': n_fft,\n",
    "              'n_mels': n_mels,\n",
    "              'hop_length': hop_length,\n",
    "              'mel_scale': 'htk',\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.mfcc = mfcc_transform(self.waveform)\n",
    "        return self.mfcc\n",
    "    \n",
    "    def get_split_mfcc(self):\n",
    "        if self.flag == 1:\n",
    "            file_name = self.name\n",
    "        else:\n",
    "            file_name = '/project/graziul/data/Zone1/2018_08_04/' + self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        self.waveform = self.waveform[:,:1800*self.sample_rate] #Clip the file at 1800s\n",
    "        clip_size = math.floor(self.waveform.shape[1]/self.n_clips)\n",
    "        n_clips = self.n_clips\n",
    "        mfcc_list = []\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "        mfcc_transform = T.MFCC(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                  'n_fft': n_fft,\n",
    "                  'n_mels': n_mels,\n",
    "                  'hop_length': hop_length,\n",
    "                  'mel_scale': 'htk',\n",
    "                }\n",
    "            )\n",
    "        for i in range(n_clips):\n",
    "            mfcc_list.append(mfcc_transform(self.waveform[:,i*clip_size:(i+1)*clip_size]))\n",
    "        self.mfcc = torch.cat(mfcc_list)\n",
    "        return self.mfcc\n",
    "    \n",
    "    def plot_waveform_with_labels(self,i,clip_size):\n",
    "        plt.figure(figsize=(14,5))\n",
    "        fig,(ax1,ax2) = plt.subplots(2,1)\n",
    "        librosa.display.waveshow(self.waveform.squeeze().numpy()[i*clip_size:(i+1)*clip_size],self.sample_rate,ax = ax1)\n",
    "        ax2.plot(self.frames_labels[i])\n",
    "        plt.show()\n",
    "        return    \n",
    "    \n",
    "    def get_plots(self): \n",
    "        clip_size = math.floor(1800*self.sample_rate/self.n_clips)\n",
    "        for i in range(self.n_clips):\n",
    "            print(i)\n",
    "            self.plot_waveform_with_labels(i,clip_size)\n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''datapath = '/project/graziul/data/whitelisted_vad_files.csv'\n",
    "dataframe = pd.read_csv(datapath, header=None)\n",
    "\n",
    "transcripts_path = '/project/graziul/transcripts/transcripts2021_10_27.csv'\n",
    "transcripts_df = pd.read_csv(transcripts_path)\n",
    "df_groups = transcripts_df.groupby(['zone','day','month','year','file'])\n",
    "#clean_transcripts_df_files = [process_filename(transcripts_file) for transcripts_file in list(transcripts_df['file'])]\n",
    "#print(clean_transcripts_df_files)\n",
    "\n",
    "new_filename_list = []\n",
    "chars_to_remove = ['_','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','.']\n",
    "for idx,elt in enumerate(list(dataframe[0])):\n",
    "    my_elt = process_filename(elt)\n",
    "    new_filename_list.append(my_elt[:-1] + '.mp3')\n",
    "    #new_filename_list.append(my_elt[:-1])\n",
    "new_filename_list = list(set(new_filename_list))\n",
    "\n",
    "to_ms = 1000\n",
    "fname_list = []\n",
    "transcripts_list = []\n",
    "for state,frame in df_groups: \n",
    "    #print(frame)\n",
    "    info_list = list(state)\n",
    "    \n",
    "    zone = info_list[0]\n",
    "    day = info_list[1]\n",
    "    month = info_list[2]\n",
    "    year = info_list[3]\n",
    "    if(len(str(day)) > 1):\n",
    "        str_day = str(day)\n",
    "    else:\n",
    "        str_day = '0' + str(day)\n",
    "    date = str(year) + '_0' + str(month) + '_' + str_day\n",
    "    filename = process_filename(info_list[4]) + '.mp3'\n",
    "    fpath = '/project/graziul/data/' + zone + '/' + date + '/' + filename\n",
    "    print(list(state))\n",
    "    print(fpath)\n",
    "    #print(filename)\n",
    "    if(is_in_list(new_filename_list, filename) == 1):\n",
    "        #print(fpath)\n",
    "        if(os.path.exists(fpath)):\n",
    "            #print(frame)\n",
    "            if(is_in_list(fname_list,fpath) == 0):\n",
    "                print(list(set(list(frame['transcriber']))))\n",
    "                #print(frame.head())\n",
    "                start_times = list(frame['start'])\n",
    "                end_times = list(frame['end'])\n",
    "                start_samples = [(int)(to_ms*start_time) for start_time in start_times]  #Convert to milliseconds\n",
    "                end_samples = [(int)(to_ms*end_time) for end_time in end_times]  #Convert to milliseconds\n",
    "                #transcripts = list(zip(start_times,end_times))\n",
    "                transcripts = list(zip(start_samples,end_samples))\n",
    "                transcripts_list.append(transcripts)\n",
    "                fname_list.append(fpath)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "new_vad_dict = {}\n",
    "for i,fname in enumerate(fname_list):\n",
    "    new_vad_dict[fname] = {'nonsilent_slices': transcripts_list[i], 'units':'milliseconds'}\n",
    "\n",
    "pkl_path = '/project/graziul/ra/ajays/whitelisted_vad_dict.pkl' \n",
    "file = open(pkl_path,'wb')\n",
    "pickle.dump(new_vad_dict,file)\n",
    "file.close()'''\n",
    "\n",
    "pkl_path = '/project/graziul/ra/ajays/whitelisted_vad_dict.pkl' \n",
    "#pkl_path = '/project/graziul/data/Zone1/2018_08_04/2018_08_04vad_dict.pkl'\n",
    "file = open(pkl_path,'rb')\n",
    "vad_dict = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "input_list = []\n",
    "labels_list = []\n",
    "\n",
    "for idx,key in enumerate(vad_dict):\n",
    "    print(idx)\n",
    "    a = audio_file(key)\n",
    "    a.get_slices(vad_dict)\n",
    "    input_list.append(a.get_split_mfcc()) \n",
    "    a.get_split_frames()\n",
    "    labels_list.append(a.get_split_labels()) \n",
    "    #a.get_plots()\n",
    "input_list = torch.cat(input_list)\n",
    "input_list = torch.transpose(input_list,1,2)\n",
    "labels_list = torch.from_numpy(np.concatenate(labels_list,axis = 0)).float()\n",
    "print(input_list.size())\n",
    "print(labels_list.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.input_dim1 = 40\n",
    "        self.input_dim2 = 64 \n",
    "        self.hidden_dim = 64\n",
    "        self.n_layers = 3\n",
    "        self.batch_size = 2\n",
    "        #(input is of format batch_size, sequence_length, num_features)\n",
    "        #hidden states should be (num_layers, batch_size, hidden_length)\n",
    "        self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = self.input_dim1, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2 = nn.LSTM(input_size = self.input_dim2, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2_out = None \n",
    "        self.hidden = None\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.convolve1d = nn.Sequential(\n",
    "            nn.Conv1d(3,3, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(3,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def create_rand_hidden1(self):\n",
    "#         self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state1, self.cell_state1)\n",
    "\n",
    "    def temp_attention(self, data):\n",
    "        #hn = self.hidden_state1\n",
    "        #cn = self.cell_state1\n",
    "        #for idx in range(data.size()[1]):\n",
    "        #    output, (hn,cn) = self.lstm1(torch.unsqueeze(data[:,idx,:],1), (hn,cn))\n",
    "        H, hidden = self.lstm1(data, (self.hidden_state1, self.cell_state1)) \n",
    "        #H = output\n",
    "        #hidden = (hn,cn)\n",
    "        H_maxtemp = torch.unsqueeze(torch.max(H, -1).values,2)\n",
    "        H_avgtemp = torch.unsqueeze(torch.mean(H, -1),2)\n",
    "        H_stdtemp = torch.unsqueeze(torch.std(H, -1),2)\n",
    "        H_concattemp = torch.cat([H_maxtemp, H_avgtemp,H_stdtemp], dim=2)\n",
    "        H_concattemp = torch.transpose(H_concattemp, 1,2)\n",
    "        return H_concattemp,H \n",
    "    \n",
    "    def convolve1(self, data):\n",
    "        H_concattemp,H = self.temp_attention(data)\n",
    "        H_temp = self.convolve1d(H_concattemp)\n",
    "        # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "        H_temp = H_temp.expand(-1,64,-1)\n",
    "        # Sigmoid activation     \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        my_input = H_temp\n",
    "        H_temp = sigmoid(my_input)\n",
    "        H_temp = torch.transpose(H_temp, 1, 2)\n",
    "        # Merge H_temp and H by element wise summation\n",
    "        H_prime = torch.stack((H,H_temp))\n",
    "        H_prime = torch.sum(H_prime,0)\n",
    "        return H_prime\n",
    "        \n",
    "#     def create_rand_hidden2(self):\n",
    "#         self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state2, self.cell_state2)  \n",
    "    \n",
    "#     def freq_attention(hidden_feature_map):\n",
    "#         H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "#         H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "#         H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "#         H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "#         return H_concatfreq \n",
    "\n",
    "    def forward(self, data):\n",
    "        input1 = self.convolve1(data)\n",
    "        #print(input1.size())\n",
    "        #hn = self.hidden_state2\n",
    "        #cn = self.cell_state2\n",
    "        #for idx in range(input1.size()[1]):\n",
    "        #    output, (hn,cn) = self.lstm2(torch.unsqueeze(input1[:,idx,:],1), (hn,cn))\n",
    "        #lstm2_out = output\n",
    "        #hidden = (hn,cn)\n",
    "        lstm2_out, hidden = self.lstm2(input1, (self.hidden_state2, self.cell_state2))\n",
    "        self.output = self.output_stack(lstm2_out)\n",
    "        print(self.output)\n",
    "        self.output = torch.squeeze(self.output)\n",
    "        return self.output\n",
    "        \n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.input_dim1 = 40\n",
    "        self.input_dim2 = 64 \n",
    "        self.hidden_dim = 64\n",
    "        self.n_layers = 3\n",
    "        self.batch_size = 2\n",
    "        #(input is of format batch_size, sequence_length, num_features)\n",
    "        #hidden states should be (num_layers, batch_size, hidden_length)\n",
    "        self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = self.input_dim1, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2 = nn.LSTM(input_size = self.input_dim2, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=True) #should be True\n",
    "        self.lstm2_out = None \n",
    "        self.hidden = None\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.convolve1d = nn.Sequential(\n",
    "            nn.Conv1d(3,3, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(3,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        out1,_ = self.lstm1(data,(self.hidden_state1,self.cell_state1))\n",
    "        out2 = self.sigmoid(self.output_stack(out1))\n",
    "        return torch.squeeze(out2)\n",
    "\n",
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=1):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha])\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=1,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        print(inputs)\n",
    "        print(targets)\n",
    "        BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        return torch.mean(BCE_loss)\n",
    "        #ce_loss = F.binary_cross_entropy(inputs, targets,reduction=self.reduction,weight=self.weight)\n",
    "        #pt = torch.exp(-ce_loss)\n",
    "        #focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        #return focal_loss\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "#model = StackedLSTM().to(device)\n",
    "model = ToyModel()\n",
    "loss_fn = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "training_steps = 30\n",
    "batch_size = model.batch_size\n",
    "num_samples = input_list.size()[0]//batch_size\n",
    "idx = 0\n",
    "flag = 0\n",
    "for step in range(training_steps):\n",
    "    input_batch = input_list[idx*batch_size:(idx+1)*batch_size]\n",
    "    labels_batch = labels_list[idx*batch_size:(idx+1)*batch_size]\n",
    "    idx = (idx+1)%num_samples\n",
    "    print(step)\n",
    "    optimizer.zero_grad()\n",
    "    output_hat = model(input_batch)\n",
    "    #print(output_hat)\n",
    "    loss = loss_fn(output_hat, labels_batch)\n",
    "    loss.backward()\n",
    "    #for param in model.parameters():\n",
    "    #    print(param.grad)\n",
    "    print(loss)\n",
    "    optimizer.step()\n",
    "    \n",
    "output_list = []\n",
    "idx = 0\n",
    "num_samples = labels_list.size()[0]//batch_size\n",
    "with torch.no_grad():\n",
    "    while(idx < num_samples):\n",
    "        print(idx)\n",
    "        input_batch = input_list[idx*batch_size:(idx+1)*batch_size]\n",
    "        labels_batch = labels_list[idx*batch_size:(idx+1)*batch_size]\n",
    "        idx = idx+1\n",
    "        output_hat = model(input_batch)\n",
    "        #print(output_hat)\n",
    "        #for param in model.parameters():\n",
    "        #    print(param.grad)\n",
    "        output_list.append(output_hat)\n",
    "    output_list = torch.cat(output_list, dim = 0)\n",
    "    \n",
    "def get_frame_error_rate(output_hat, labels):\n",
    "    num_samples = labels.size()[0]\n",
    "    fer_arr = []\n",
    "    for i in range(num_samples):\n",
    "        curr_output = output_hat[i]\n",
    "        curr_label = labels[i]\n",
    "        fer_arr.append(torch.mean(torch.add(curr_output,curr_label)%2).data*100)\n",
    "    return fer_arr\n",
    "\n",
    "def test_frame_error_rate(output_hat, labels):\n",
    "    num_samples = labels.size()[0]\n",
    "    s_length = labels.size()[1]\n",
    "    fer_arr = []\n",
    "    sum = 0\n",
    "    for i in range(num_samples):\n",
    "        curr_output = output_hat[i]\n",
    "        curr_label = labels[i]\n",
    "        for j in range(s_length):\n",
    "            if curr_output[j] == curr_label[j]:\n",
    "                pass\n",
    "            else:\n",
    "                sum = sum+1\n",
    "        fer_arr.append(torch.mean(torch.add(curr_output,curr_label)%2)*100)\n",
    "    return sum\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Frame error Rate :\" + str(get_frame_error_rate(torch.round(output_list),labels_list)))\n",
    "    print(test_frame_error_rate(output_list, labels_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajay_env",
   "language": "python",
   "name": "ajay_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
