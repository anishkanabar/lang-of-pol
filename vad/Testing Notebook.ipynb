{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f6b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.5324, 0.5431, 0.5024,  ..., 0.5312, 0.5380, 0.5326],\n",
      "        [0.5259, 0.5436, 0.5022,  ..., 0.5245, 0.5241, 0.5058]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.7259, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.2342209815979004\n",
      "1\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4094, 0.2236, 0.4213,  ..., 0.4120, 0.3607, 0.2848],\n",
      "        [0.3454, 0.1940, 0.3944,  ..., 0.4257, 0.3473, 0.2807]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.2196, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.3978078365325928\n",
      "2\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.2954, 0.2022, 0.3237,  ..., 0.2971, 0.1834, 0.2548],\n",
      "        [0.3020, 0.2067, 0.3266,  ..., 0.2910, 0.1841, 0.2497]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.0327, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4273271560668945\n",
      "3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.2827, 0.1701, 0.1524,  ..., 0.1282, 0.0126, 0.0200],\n",
      "        [0.2497, 0.1347, 0.1507,  ..., 0.1243, 0.0158, 0.0345]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6810, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.5430400371551514\n",
      "4\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.4012, 0.3826, 0.3640,  ..., 0.1872, 0.0449, 0.0219],\n",
      "        [0.0906, 0.0697, 0.0405,  ..., 0.1755, 0.0339, 0.0172]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.8473, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.481611728668213\n",
      "0\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4655, 0.4528, 0.4448,  ..., 0.4418, 0.4500, 0.4618],\n",
      "        [0.4639, 0.4386, 0.4466,  ..., 0.4458, 0.4526, 0.4540]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6572, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4446260929107666\n",
      "1\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.4231, 0.3620, 0.3188,  ..., 0.0323, 0.0625, 0.0771],\n",
      "        [0.4322, 0.3332, 0.2602,  ..., 0.0491, 0.0589, 0.0659]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.7445, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4536960124969482\n",
      "2\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4454, 0.3774, 0.3860,  ..., 0.3666, 0.1752, 0.3180],\n",
      "        [0.4435, 0.3710, 0.3825,  ..., 0.1361, 0.1655, 0.1862]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.8052, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4346301555633545\n",
      "3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.3974, 0.3432, 0.3342,  ..., 0.4593, 0.4159, 0.4415],\n",
      "        [0.3981, 0.3440, 0.3339,  ..., 0.4656, 0.4191, 0.4586]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.4254, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4772677421569824\n",
      "4\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.3255, 0.2641, 0.2870,  ..., 0.4582, 0.4505, 0.4255],\n",
      "        [0.3297, 0.2644, 0.2839,  ..., 0.3321, 0.3286, 0.3493]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.5403, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4400312900543213\n",
      "0\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.5167, 0.5195, 0.5110,  ..., 0.5160, 0.5130, 0.5069],\n",
      "        [0.5187, 0.5219, 0.5133,  ..., 0.5174, 0.5154, 0.5076]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6984, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4302124977111816\n",
      "1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.2100, 0.1178, 0.1555,  ..., 0.2751, 0.2470, 0.2506],\n",
      "        [0.3755, 0.2970, 0.3990,  ..., 0.2798, 0.2449, 0.2464]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.9365, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4258124828338623\n",
      "2\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4441, 0.4419, 0.4661,  ..., 0.3808, 0.3977, 0.4350],\n",
      "        [0.4402, 0.4465, 0.4746,  ..., 0.3737, 0.3971, 0.4335]],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "import util\n",
    "from util import audio_file\n",
    "from util import *\n",
    "from Losses import FocalLoss,WeightedFocalLoss\n",
    "from StackedLSTM2 import StackedLSTM\n",
    "from AttentionLSTM import Attention_LSTM\n",
    "from Toy_Model import ToyModel\n",
    "import time\n",
    "torch.manual_seed(1)\n",
    "\n",
    "my_dataset = \"ATC0\"\n",
    "my_model = \"Attention_LSTM\"\n",
    "verbose = 1\n",
    "\n",
    "'''if my_dataset == \"ATC0\":\n",
    "    input_list, labels_list = process_atc0_files(2)\n",
    "    \n",
    "elif my_dataset == \"BPC\":\n",
    "    input_list, labels_list = load_data_limit(5)\n",
    "\n",
    "n_samples = input_list.size()[0]\n",
    "train_split = 4*n_samples//5\n",
    "test_samples = n_samples - train_split\n",
    "    \n",
    "test_input_list = input_list[train_split:]\n",
    "test_labels_list = labels_list[train_split:]\n",
    "input_list = input_list[:train_split]\n",
    "labels_list = labels_list[:train_split]\n",
    "\n",
    "if my_model == \"Attention_LSTM\":\n",
    "    model = Attention_LSTM()\n",
    "    save_filepath = '/project/graziul/ra/ajays/LSTM_model_predictions.txt'\n",
    "elif my_model == \"Vanilla_LSTM\":\n",
    "    model = ToyModel()\n",
    "    save_filepath = '/project/graziul/ra/ajays/toy_model_predictions.txt'\n",
    "'''\n",
    "sample_size = 30\n",
    "batch_size = model.batch_size\n",
    "num_samples = input_list.size()[0]//batch_size\n",
    "training_steps = 5\n",
    "idx = 0\n",
    "flag = 0\n",
    "num_segments = 30\n",
    "val_size = 30\n",
    "gammas = [0.0001,0.001,0.005,0.1,1,10]\n",
    "\n",
    "for gamma in gammas:\n",
    "    train_loss_list = []\n",
    "    loss_fn = FocalLoss(gamma = gamma)\n",
    "    if my_model == \"Attention_LSTM\":\n",
    "        model = Attention_LSTM()\n",
    "        save_filepath = '/project/graziul/ra/ajays/LSTM_model_predictions.txt'\n",
    "    elif my_model == \"Vanilla_LSTM\":\n",
    "        model = ToyModel()\n",
    "        save_filepath = '/project/graziul/ra/ajays/toy_model_predictions.txt'\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    fer_list = []\n",
    "\n",
    "    test_loss_list = []\n",
    "\n",
    "    for step in range(training_steps):\n",
    "        start_time = time.time()\n",
    "        input_batch = input_list[idx*batch_size:(idx+1)*batch_size]\n",
    "        labels_batch = labels_list[idx*batch_size:(idx+1)*batch_size]\n",
    "        idx = (idx+1)%num_samples\n",
    "        print(step)\n",
    "        optimizer.zero_grad()\n",
    "        output_hat = model(input_batch)\n",
    "        #print(output_hat)\n",
    "        print(labels_batch)\n",
    "        print(output_hat)\n",
    "        loss = loss_fn(output_hat, labels_batch)\n",
    "        loss.backward()\n",
    "        #for param in model.parameters():\n",
    "        #    print(param.grad)\n",
    "        print(loss)\n",
    "        train_loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        step_time = end_time - start_time\n",
    "        print(\"Time Taken for Step = \" + str(step_time))\n",
    "\n",
    "    plt.plot(list(range(training_steps)),train_loss_list, label = str(gamma))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df6cc76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/jobs/3601942/ipykernel_3230211/2271843605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*.sph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "#Threshold on ATC0\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "import util\n",
    "from util import audio_file\n",
    "from util import *\n",
    "from Losses import FocalLoss,WeightedFocalLoss\n",
    "from StackedLSTM2 import StackedLSTM\n",
    "from AttentionLSTM import Attention_LSTM\n",
    "from Toy_Model import ToyModel\n",
    "import time\n",
    "\n",
    "zcr_list = []\n",
    "rms_list = []\n",
    "autocorrelation_list = []\n",
    "labels_list = []\n",
    "audio_file_arr = []\n",
    "frame_size = 25\n",
    "frame_hop = 10\n",
    "idx = 0\n",
    "transcript_list_arr = []\n",
    "min_zcr_t = 0\n",
    "min_rms_t = 0\n",
    "max_zcr_t = 10\n",
    "max_rms_t = 10\n",
    "zcr_t_arr = np.linspace(min_zcr_t,max_zcr_t,20)\n",
    "#zcr_threshold = 1\n",
    "rms_t_arr = np.linspace(min_rms_t,max_rms_t,20)\n",
    "\n",
    "input_list = []\n",
    "labels_list = []\n",
    "txtfile = open('/project/graziul/ra/ajays/stats_atc0.txt','w')\n",
    "txtfile.truncate(0)\n",
    "#paths = ['/project/graziul/data/corpora/atc0_comp/atc0_bos/data/audio/', '/project/graziul/data/corpora/atc0_comp/atc0_dca/data/audio/', '/project/graziul/data/corpora/atc0_comp/atc0_dfw/data/audio/']\n",
    "paths = ['/project/graziul/data/corpora/atc0_comp/atc0_bos/data/audio/']\n",
    "for idx,path in enumerate(paths):\n",
    "    for fpath in glob(path + '*.sph'):\n",
    "        if(idx > k):\n",
    "            break\n",
    "        filename = fpath[-12:-4]\n",
    "        label_file = path[:-6] + 'transcripts/' + filename + '.txt'\n",
    "        waveform, sample_rate = torchaudio.load(file_name)\n",
    "        effects = [['rate', '22050']]\n",
    "        x,sr = torchaudio.sox_effects.apply_effects_tensor(waveform, sample_rate, effects)\n",
    "        with open(label_file, 'r') as f:\n",
    "            label = f.read()\n",
    "        y = process_atc_label_line(label)\n",
    "        \n",
    "        input_list.append(x)\n",
    "        labels_list.append(y)\n",
    "        idx = idx+1\n",
    "        zcr = np.array(np.squeeze(get_zcr(x,sr, frame_length=sr*frame_size,hop_length=sr*frame_hop)))\n",
    "        zcr_list.append(zcr)\n",
    "        rms = np.array(np.squeeze(librosa.feature.rms(x,frame_length=sr*frame_size,hop_length=sr*frame_hop)))\n",
    "        rms_list.append(rms)\n",
    "        get_plots(x,zcr,rms)\n",
    "        frame_error_rate_list = []\n",
    "        for zcr_threshold in zcr_t_arr:\n",
    "            for rms_threshold in rms_t_arr:\n",
    "                frame_error_rate = process_single_file(x,label,sr,zcr_threshold,rms_threshold)\n",
    "                frame_error_rate_list.append(frame_error_rate)\n",
    "                writeline = audio_file + ' ' + str(zcr_threshold) + ' ' + str(rms_threshold) + ' ' + str(frame_error_rate) + '\\n'\n",
    "                txtfile.write(writeline)\n",
    "        frame_error_rate_list = np.array(frame_error_rate_list)\n",
    "        get3dplots(frame_error_rate_list, zcr_t_arr, rms_t_arr,idx)       \n",
    "        \n",
    "    if(idx>k):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_torch_env",
   "language": "python",
   "name": "new_torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
