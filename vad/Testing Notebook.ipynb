{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f6b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.5324, 0.5431, 0.5024,  ..., 0.5312, 0.5380, 0.5326],\n",
      "        [0.5259, 0.5436, 0.5022,  ..., 0.5245, 0.5241, 0.5058]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.7259, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.2342209815979004\n",
      "1\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4094, 0.2236, 0.4213,  ..., 0.4120, 0.3607, 0.2848],\n",
      "        [0.3454, 0.1940, 0.3944,  ..., 0.4257, 0.3473, 0.2807]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.2196, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.3978078365325928\n",
      "2\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.2954, 0.2022, 0.3237,  ..., 0.2971, 0.1834, 0.2548],\n",
      "        [0.3020, 0.2067, 0.3266,  ..., 0.2910, 0.1841, 0.2497]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.0327, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4273271560668945\n",
      "3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.2827, 0.1701, 0.1524,  ..., 0.1282, 0.0126, 0.0200],\n",
      "        [0.2497, 0.1347, 0.1507,  ..., 0.1243, 0.0158, 0.0345]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6810, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.5430400371551514\n",
      "4\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.4012, 0.3826, 0.3640,  ..., 0.1872, 0.0449, 0.0219],\n",
      "        [0.0906, 0.0697, 0.0405,  ..., 0.1755, 0.0339, 0.0172]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.8473, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.481611728668213\n",
      "0\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4655, 0.4528, 0.4448,  ..., 0.4418, 0.4500, 0.4618],\n",
      "        [0.4639, 0.4386, 0.4466,  ..., 0.4458, 0.4526, 0.4540]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6572, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4446260929107666\n",
      "1\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[0.4231, 0.3620, 0.3188,  ..., 0.0323, 0.0625, 0.0771],\n",
      "        [0.4322, 0.3332, 0.2602,  ..., 0.0491, 0.0589, 0.0659]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.7445, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4536960124969482\n",
      "2\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4454, 0.3774, 0.3860,  ..., 0.3666, 0.1752, 0.3180],\n",
      "        [0.4435, 0.3710, 0.3825,  ..., 0.1361, 0.1655, 0.1862]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.8052, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4346301555633545\n",
      "3\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.3974, 0.3432, 0.3342,  ..., 0.4593, 0.4159, 0.4415],\n",
      "        [0.3981, 0.3440, 0.3339,  ..., 0.4656, 0.4191, 0.4586]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.4254, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4772677421569824\n",
      "4\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.3255, 0.2641, 0.2870,  ..., 0.4582, 0.4505, 0.4255],\n",
      "        [0.3297, 0.2644, 0.2839,  ..., 0.3321, 0.3286, 0.3493]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.5403, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4400312900543213\n",
      "0\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.5167, 0.5195, 0.5110,  ..., 0.5160, 0.5130, 0.5069],\n",
      "        [0.5187, 0.5219, 0.5133,  ..., 0.5174, 0.5154, 0.5076]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.6984, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4302124977111816\n",
      "1\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.2100, 0.1178, 0.1555,  ..., 0.2751, 0.2470, 0.2506],\n",
      "        [0.3755, 0.2970, 0.3990,  ..., 0.2798, 0.2449, 0.2464]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor(0.9365, grad_fn=<MeanBackward0>)\n",
      "Time Taken for Step = 2.4258124828338623\n",
      "2\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0.4441, 0.4419, 0.4661,  ..., 0.3808, 0.3977, 0.4350],\n",
      "        [0.4402, 0.4465, 0.4746,  ..., 0.3737, 0.3971, 0.4335]],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "import util\n",
    "from util import audio_file\n",
    "from util import *\n",
    "from Losses import FocalLoss,WeightedFocalLoss\n",
    "from StackedLSTM2 import StackedLSTM\n",
    "from AttentionLSTM import Attention_LSTM\n",
    "from Toy_Model import ToyModel\n",
    "import time\n",
    "torch.manual_seed(1)\n",
    "\n",
    "my_dataset = \"ATC0\"\n",
    "my_model = \"Attention_LSTM\"\n",
    "verbose = 1\n",
    "\n",
    "'''if my_dataset == \"ATC0\":\n",
    "    input_list, labels_list = process_atc0_files(2)\n",
    "    \n",
    "elif my_dataset == \"BPC\":\n",
    "    input_list, labels_list = load_data_limit(5)\n",
    "\n",
    "n_samples = input_list.size()[0]\n",
    "train_split = 4*n_samples//5\n",
    "test_samples = n_samples - train_split\n",
    "    \n",
    "test_input_list = input_list[train_split:]\n",
    "test_labels_list = labels_list[train_split:]\n",
    "input_list = input_list[:train_split]\n",
    "labels_list = labels_list[:train_split]\n",
    "\n",
    "if my_model == \"Attention_LSTM\":\n",
    "    model = Attention_LSTM()\n",
    "    save_filepath = '/project/graziul/ra/ajays/LSTM_model_predictions.txt'\n",
    "elif my_model == \"Vanilla_LSTM\":\n",
    "    model = ToyModel()\n",
    "    save_filepath = '/project/graziul/ra/ajays/toy_model_predictions.txt'\n",
    "'''\n",
    "sample_size = 30\n",
    "batch_size = model.batch_size\n",
    "num_samples = input_list.size()[0]//batch_size\n",
    "training_steps = 5\n",
    "idx = 0\n",
    "flag = 0\n",
    "num_segments = 30\n",
    "val_size = 30\n",
    "gammas = [0.0001,0.001,0.005,0.1,1,10]\n",
    "\n",
    "for gamma in gammas:\n",
    "    train_loss_list = []\n",
    "    loss_fn = FocalLoss(gamma = gamma)\n",
    "    if my_model == \"Attention_LSTM\":\n",
    "        model = Attention_LSTM()\n",
    "        save_filepath = '/project/graziul/ra/ajays/LSTM_model_predictions.txt'\n",
    "    elif my_model == \"Vanilla_LSTM\":\n",
    "        model = ToyModel()\n",
    "        save_filepath = '/project/graziul/ra/ajays/toy_model_predictions.txt'\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    fer_list = []\n",
    "\n",
    "    test_loss_list = []\n",
    "\n",
    "    for step in range(training_steps):\n",
    "        start_time = time.time()\n",
    "        input_batch = input_list[idx*batch_size:(idx+1)*batch_size]\n",
    "        labels_batch = labels_list[idx*batch_size:(idx+1)*batch_size]\n",
    "        idx = (idx+1)%num_samples\n",
    "        print(step)\n",
    "        optimizer.zero_grad()\n",
    "        output_hat = model(input_batch)\n",
    "        #print(output_hat)\n",
    "        print(labels_batch)\n",
    "        print(output_hat)\n",
    "        loss = loss_fn(output_hat, labels_batch)\n",
    "        loss.backward()\n",
    "        #for param in model.parameters():\n",
    "        #    print(param.grad)\n",
    "        print(loss)\n",
    "        train_loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        step_time = end_time - start_time\n",
    "        print(\"Time Taken for Step = \" + str(step_time))\n",
    "\n",
    "    plt.plot(list(range(training_steps)),train_loss_list, label = str(gamma))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cfbe5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/graziul/data/corpora/atc0_comp/atc0_bos/data/audio/log_sm_1.sph\n"
     ]
    }
   ],
   "source": [
    "#Threshold on ATC0\n",
    "def get_zcr(x,sr,frame_length=25,hop_length=10, n_divs = 30):\n",
    "    #Assume x is 1 dimensional\n",
    "    shape = x.size\n",
    "    cross_list = []\n",
    "    div_size = shape//n_divs\n",
    "    for i in range(n_divs):\n",
    "        inputs = x[i*div_size:(i+1)*div_size]\n",
    "        frame_arr = librosa.util.frame(inputs,frame_length,hop_length,axis = 0)\n",
    "        temp_cross_arr = librosa.zero_crossings(frame_arr, threshold = -1)\n",
    "        cross_list.append(temp_cross_arr)\n",
    "    cross_arr = np.concatenate(cross_list)\n",
    "    return np.mean(cross_arr, axis = -1)\n",
    "\n",
    "def get_rms(x,sr,frame_length=25,hop_length=10, n_divs = 30):\n",
    "    shape = x.size\n",
    "    rms_list = []\n",
    "    div_size = shape//n_divs\n",
    "    for i in range(n_divs):\n",
    "        inputs = x[i*div_size:(i+1)*div_size]\n",
    "        rms_list.append(librosa.feature.rms(inputs,frame_length=frame_length,hop_length = hop_length))\n",
    "    return np.concatenate(rms_list)\n",
    "\n",
    "def unionise(transcript_list): #Collapse intervals like (1,2),(2,3) into (1,3)\n",
    "    new_list = []\n",
    "    for interval in transcript_list:\n",
    "        start_pt = interval[0]\n",
    "        end_pt = interval[1]\n",
    "        final_elt = [-2,-1]\n",
    "        if new_list:\n",
    "            final_elt = new_list[-1]\n",
    "        if final_elt[1] == start_pt:\n",
    "            new_list[-1] = [final_elt[0],end_pt]\n",
    "        else:\n",
    "            new_list.append([start_pt,end_pt])\n",
    "    return new_list\n",
    "\n",
    "def get_transcripts(zcr_arr,rms_arr,zcr_t,rms_t, frame_length = 25, frame_hop=10,sr = 22050, min_interval = 1): \n",
    "    #Returns the transcripts given by the threshold algorithm. zcr_t and rms_t are the thresholds. It returns those\n",
    "    # as voice segments which satisfy both the zcr threshold AND the rms threshold\n",
    "    transcript_list = []\n",
    "    on_flag = 0\n",
    "    for i in range(zcr_arr.size):\n",
    "        if((zcr_arr[i] <= zcr_t and rms_arr[i] > rms_t) and on_flag== 0):\n",
    "            start_time = float(math.floor((frame_hop*i/sr))) #convert everything to seconds\n",
    "            on_flag = 1\n",
    "        if((zcr_arr[i] > zcr_t or rms_arr[i] <= rms_t) and on_flag == 1):\n",
    "            end_time= float(math.floor(((frame_hop*(i-1)/sr)))) #convert everything to seconds\n",
    "            if (end_time - start_time) > min_interval:\n",
    "                on_flag = 0\n",
    "                transcript_list.append([start_time,end_time])\n",
    "    return unionise(transcript_list)\n",
    "\n",
    "def is_in_interval(arr,idx): #arr is an array with each element being a tuple corresponding to an interval\n",
    "    #function checks whether idx is in any interval in the list\n",
    "    if not arr:\n",
    "        return 0\n",
    "    for pair in arr:\n",
    "        start = pair[0]\n",
    "        end = pair[1]\n",
    "        if(start <= idx and end >=idx):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def get_tframe_error_rate(x,y_hat, y,sr = 22050):\n",
    "    #Gets frame error rate - sums false positives and false negatives\n",
    "    n = int(x.size//sr)\n",
    "    err = 0\n",
    "    for i in range(n):\n",
    "        if(is_in_interval(y_hat,i) + is_in_interval(y,i) == 1): #This if condition means it is either a false positive or false negative - exactly one of the two is in the interval\n",
    "            err = err + 1\n",
    "    return 100*(err/n)\n",
    "\n",
    "def get_plots(x,zcr,rms,index = 0, time_slice = 1000000, t = 45): #plots audio, zcr, and rms for one sample\n",
    "    #The time indices used are from index*time_slice to (index+1)*time_slice\n",
    "    print(\"New Sample\")\n",
    "    jump = int(sr*t//10)\n",
    "    plt.figure(figsize=(14,5))\n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(3,1)\n",
    "    librosa.display.waveshow(x[index*time_slice:(index+1)*time_slice],sr, ax = ax1)\n",
    "    ax2.plot(rms[index*jump:(index+1)*jump])\n",
    "    ax3.plot(zcr[index*jump:(index+1)*jump])\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_stats(x,zcr,rms, bins = 10):\n",
    "    get_plots(x,zcr,rms)\n",
    "    softening = 1e-9\n",
    "    print(\"Stats for ZCR:\")\n",
    "    print(pd.DataFrame(zcr).describe())\n",
    "    print(\"Stats for RMS:\")\n",
    "    print(pd.DataFrame(rms).describe())\n",
    "    plt.figure(figsize=(14,5))\n",
    "    fig,(ax1,ax2) = plt.subplots(2,1)\n",
    "    ax1.hist(np.histogram(zcr, bins = bins, density = True), bins = bins)\n",
    "    print(\"Histogram for RMS:\")\n",
    "    ax2.hist(np.histogram(rms, bins = bins, density = True), bins = bins)\n",
    "    return\n",
    "\n",
    "\n",
    "def get3dplots(frame_error_rate_list,zcr_threshold_list,rms_threshold_list,iteration, file_extension = '.png'):\n",
    "    fig = plt.figure(figsize=(14,5))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    n = len(zcr_threshold_list)\n",
    "    m = len(rms_threshold_list)\n",
    "    my_idx = 0\n",
    "    #for zcr_elt in zcr_threshold_list:\n",
    "    #    for rms_elt in rms_threshold_list:\n",
    "    #        ax.scatter3D(zcr_elt, rms_elt, frame_error_rate_list[my_idx])\n",
    "    #        my_idx = my_idx+1\n",
    "    x1,y1 = np.meshgrid((zcr_threshold_list), (rms_threshold_list))\n",
    "    print(np.shape((zcr_threshold_list)), np.shape((rms_threshold_list)), np.shape(frame_error_rate_list))\n",
    "    ax.plot_surface(x1,y1,frame_error_rate_list.reshape([m,n]),cmap = 'Spectral')\n",
    "    ax.set_xlabel(\"ZCR-Threshold\")\n",
    "    ax.set_ylabel(\"RMS-Threshold\")\n",
    "    ax.set_zlabel(\"Frame Error Rate\")\n",
    "    ax.set_title(\"Frame Error Rate for Different Thresholds\")\n",
    "    plt.show()\n",
    "    plt.savefig('/project/graziul/ra/ajays/threshold_plots/frame_error_rate_' + str(iteration) + file_extension)\n",
    "    return\n",
    "\n",
    "def process_single_file(x, label, sr, zcr_threshold, rms_threshold):\n",
    "    print(\"ZCR Threshold = \" + str(zcr_threshold) + \" RMS Threshold = \" + str(rms_threshold))\n",
    "    transcript_list = get_transcripts(zcr,rms,zcr_threshold,rms_threshold,frame_size*sr,frame_hop*sr,sr, 0)\n",
    "    print(\"The frame error rate is: \", end = '')\n",
    "    print(x.shape, transcript_list.shape)\n",
    "    frame_error_rate = get_tframe_error_rate(x,transcript_list, label,sr)\n",
    "    print(frame_error_rate)\n",
    "    return frame_error_rate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "import util\n",
    "from util import audio_file\n",
    "from util import *\n",
    "from Losses import FocalLoss,WeightedFocalLoss\n",
    "from StackedLSTM2 import StackedLSTM\n",
    "from AttentionLSTM import Attention_LSTM\n",
    "from Toy_Model import ToyModel\n",
    "import time\n",
    "\n",
    "zcr_list = []\n",
    "rms_list = []\n",
    "autocorrelation_list = []\n",
    "labels_list = []\n",
    "audio_file_arr = []\n",
    "frame_size = 25\n",
    "frame_hop = 10\n",
    "idx = 0\n",
    "transcript_list_arr = []\n",
    "min_zcr_t = 0\n",
    "min_rms_t = 0\n",
    "max_zcr_t = 10\n",
    "max_rms_t = 10\n",
    "zcr_t_arr = np.linspace(min_zcr_t,max_zcr_t,20)\n",
    "#zcr_threshold = 1\n",
    "rms_t_arr = np.linspace(min_rms_t,max_rms_t,20)\n",
    "#k = 1000000\n",
    "k = 3\n",
    "input_list = []\n",
    "labels_list = []\n",
    "txtfile = open('/project/graziul/ra/ajays/stats_atc0.txt','w')\n",
    "txtfile.truncate(0)\n",
    "#paths = ['/project/graziul/data/corpora/atc0_comp/atc0_bos/data/audio/', '/project/graziul/data/corpora/atc0_comp/atc0_dca/data/audio/', '/project/graziul/data/corpora/atc0_comp/atc0_dfw/data/audio/']\n",
    "paths = ['/project/graziul/data/corpora/atc0_comp/atc0_bos/data/audio/']\n",
    "for idx,path in enumerate(paths):\n",
    "    for fpath in glob(path + '*.sph'):\n",
    "        if(idx > k):\n",
    "            break\n",
    "        print(fpath)\n",
    "        filename = fpath[-12:-4]\n",
    "        label_file = path[:-6] + 'transcripts/' + filename + '.txt'\n",
    "        waveform, sample_rate = torchaudio.load(fpath)\n",
    "        effects = [['rate', '22050']]\n",
    "        x,sr = torchaudio.sox_effects.apply_effects_tensor(waveform, sample_rate, effects)\n",
    "        with open(label_file, 'r') as f:\n",
    "            label = f.read()\n",
    "        y = process_atc_label_line(label)\n",
    "        x = x.squeeze().numpy()\n",
    "        print(x.shape)\n",
    "        input_list.append(x)\n",
    "        labels_list.append(y)\n",
    "        idx = idx+1\n",
    "        zcr = np.array(np.squeeze(get_zcr(x,sr, frame_length=sr*frame_size,hop_length=sr*frame_hop)))\n",
    "        zcr_list.append(zcr)\n",
    "        #rms = np.array(np.squeeze(librosa.feature.rms(x,frame_length=sr*frame_size,hop_length=sr*frame_hop)))\n",
    "        rms = np.array(np.squeeze(get_rms(x,sr,frame_length=sr*frame_size,hop_length=sr*frame_hop)))\n",
    "        rms_list.append(rms)\n",
    "        get_plots(x,zcr,rms)\n",
    "        frame_error_rate_list = []\n",
    "        for zcr_threshold in zcr_t_arr:\n",
    "            for rms_threshold in rms_t_arr:\n",
    "                frame_error_rate = process_single_file(x,label,sr,zcr_threshold,rms_threshold)\n",
    "                frame_error_rate_list.append(frame_error_rate)\n",
    "                writeline = audio_file + ' ' + str(zcr_threshold) + ' ' + str(rms_threshold) + ' ' + str(frame_error_rate) + '\\n'\n",
    "                txtfile.write(writeline)\n",
    "        frame_error_rate_list = np.array(frame_error_rate_list)\n",
    "        get3dplots(frame_error_rate_list, zcr_t_arr, rms_t_arr,idx)       \n",
    "        \n",
    "    if(idx>k):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_torch_env",
   "language": "python",
   "name": "new_torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
